# -*- coding: utf-8 -*-
"""mohamed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_sS-h7oeWiGGtLFNSczLxXridgbcQs6u

# ## Table des mati√®res

 1.  **Initialisation et Pr√©paration des R√©pertoires**
 2.  **Pr√©traitement des Donn√©es et D√©coupage**
 3.  **Impl√©mentation de Mask R-CNN**
 4.  **Impl√©mentation de UNet++ Multiclasse vec Approche XOR**
 5.  **Optimisation et Nettoyage du Code**

**üß† Partie 1 ‚Äî Initialisation et Pr√©paration des R√©pertoires**

Cette √©tape pr√©pare l'environnement, monte Google Drive, configure les chemins et g√©n√®re les r√©pertoires n√©cessaires pour organiser les images, masques et jeux de donn√©es (train/val/test).

üì¶ 1. Installation et Importation des Biblioth√®ques
"""

!pip install numpy

!pip install numpy opencv-python

# Importation des biblioth√®ques utiles
import numpy as np
import cv2
import os
from pathlib import Path

"""2. Montage de Google Drive

1. T√©l√©verser le fichier .json sur Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""üìÅ 3. D√©finition des chemins principaux"""

import os
# Dossier racine des donn√©es dans Google Drive
ROOT_DIR = '/content/drive/MyDrive/Colab_Notebooks/new_data'
print("Contenu du dossier racine :", os.listdir(ROOT_DIR))

# Dossiers d‚Äôimages et masques originaux
IMG_ROOT_ORIGINAL = os.path.join(ROOT_DIR, "Images_converted", "MCF10A-3000Cells")
MSK_ROOT_ORIGINAL = os.path.join(ROOT_DIR, "ManualSegmentations_converted", "MCF10A-3000Cells")

RESIZED_IMG_DIR = os.path.join(ROOT_DIR, "ResizedImgs")
RESIZED_MSK_DIR = os.path.join(ROOT_DIR, "ResizedMasks")

# Jeux de donn√©es pour l‚Äôapprentissage
DATASET_SPLIT_DIR = os.path.join(ROOT_DIR, "dataset_split")
TRAIN_DIR = os.path.join(DATASET_SPLIT_DIR, "train")
VAL_DIR = os.path.join(DATASET_SPLIT_DIR, "val")
TEST_DIR = os.path.join(DATASET_SPLIT_DIR, "test")

# Param√®tres globaux
CLASSES_DICT = {
    1: "monocouche",
    2: "aggregation",
    3: "spheroide"
}

CLASSES = list(CLASSES_DICT.values())

# Pour Mask R-CNN (fond + monocouche + agr√©gation + sph√©ro√Øde)
NUM_CLASSES_ALL = len(CLASSES) + 1 # 0=fond, 1=monocouche, 2=agr√©gation, 3=sph√©ro√Øde
# Pour U-Net++ multiclasse (fond + monocouche + agr√©gation) - sph√©ro√Ødes remapp√©s au fond

# === MultiClasse ===
CLASSES_MULTICLASS = ["aggregation", "monocouche"]  # üëâ Seulement ces classes
NUM_CLASSES_UNET_MULTICLASS = len(CLASSES_MULTICLASS) + 1  # +1 pour le fond (0)

# === Binaire ===
CLASSES_BINARY = ["spheroide"]  # üëâ Seulement spheroide
SPHEROID_CLASS_ID = 3

TARGET_SIZE = (256, 256)  # Pour le redimensionnement

"""4: Configuration des mod√®les

"""

IMAGE_SIZE = (256,256)
BATCH_SIZE = 1 # R√©duit pour √©conomiser la m√©moire
LEARNING_RATE = 1e-4
MAX_EPOCHS = 30

"""üóÇÔ∏è 5. Cr√©ation des R√©pertoires de Travail

"""

def create_directories(ROOT_DIR, CLASSES,RESIZED_IMG_DIR, RESIZED_MSK_DIR,
                       TRAIN_DIR, VAL_DIR, TEST_DIR,
                       IMG_ROOT_ORIGINAL):

    print("\nüìå Chemin racine :", ROOT_DIR)
    if not os.path.exists(ROOT_DIR):
        raise FileNotFoundError(f"‚ùå Le r√©pertoire {ROOT_DIR} n'existe pas.")

    # V√©rification du dossier d‚Äôimages brutes
    print("\nüìÅ Contenu de Images/MCF10A-3000Cells :")
    if os.path.isdir(IMG_ROOT_ORIGINAL):
        for cls in os.listdir(IMG_ROOT_ORIGINAL):
            print(" ‚îú‚îÄ", cls)
    else:
        print(" ‚ö†Ô∏è  Dossier non trouv√© :", IMG_ROOT_ORIGINAL)
 # 2. Dossiers pour images et masques redimensionn√©s
    for cls in CLASSES:
        os.makedirs(os.path.join(RESIZED_IMG_DIR, cls), exist_ok=True)
        os.makedirs(os.path.join(RESIZED_MSK_DIR, cls), exist_ok=True)
        print(f"üìè Dossier Redimensionn√© : {cls}")

    # 4. Dossiers pour les splits train/val/test
    for split_dir in [TRAIN_DIR, VAL_DIR, TEST_DIR]:
        for sub_dir in ["images", "masks"]:
            for cls in CLASSES:
                path = os.path.join(split_dir, sub_dir, cls)
                os.makedirs(path, exist_ok=True)
                print(f"üìÇ Dossier {split_dir}/{sub_dir}/{cls} pr√™t.")

"""‚úÖ 6. Lancement de la cr√©ation des dossiers"""

create_directories(ROOT_DIR, CLASSES,RESIZED_IMG_DIR, RESIZED_MSK_DIR,
                       TRAIN_DIR, VAL_DIR, TEST_DIR,
                       IMG_ROOT_ORIGINAL)

print("\n‚úÖ Tous les r√©pertoires n√©cessaires ont √©t√© cr√©√©s.")

"""**üß™ Partie 2 ‚Äî Pr√©traitement des Donn√©es et D√©coupage**"""

import os
import cv2
import glob
import numpy as np

print(f"\nüîÑ Redimensionnement des images et masques √† {TARGET_SIZE[0]}x{TARGET_SIZE[1]}‚Ä¶")

for cls in CLASSES:
    print(f"\nüîÑ Traitement de la classe '{cls}'‚Ä¶")

    img_folder = os.path.join(IMG_ROOT_ORIGINAL, cls)
    msk_folder = os.path.join(MSK_ROOT_ORIGINAL, cls)
    out_img_cls = os.path.join(RESIZED_IMG_DIR, cls)
    out_msk_cls = os.path.join(RESIZED_MSK_DIR, cls)

    os.makedirs(out_img_cls, exist_ok=True)
    os.makedirs(out_msk_cls, exist_ok=True)

    img_paths = sorted(glob.glob(os.path.join(img_folder, "*.png")))
    print(f"  ‚Ä¢ {len(img_paths)} image(s) √† redimensionner.")

    for img_path in img_paths:
        base = os.path.splitext(os.path.basename(img_path))[0]
        mask_path = os.path.join(msk_folder, base + ".png")  # <- modifi√© ici

        img = cv2.imread(img_path)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        if img is None:
            print(f"  ‚ö†Ô∏è Image introuvable ou invalide: {img_path}")
            continue
        if mask is None:
            print(f"  ‚ö†Ô∏è Masque introuvable ou invalide: {mask_path}")
            continue

        img_resized = cv2.resize(img, TARGET_SIZE, interpolation=cv2.INTER_AREA)
        mask_resized = cv2.resize(mask, TARGET_SIZE, interpolation=cv2.INTER_NEAREST)

        cv2.imwrite(os.path.join(out_img_cls, os.path.basename(img_path)), img_resized)
        cv2.imwrite(os.path.join(out_msk_cls, os.path.basename(mask_path)), mask_resized)

print("\n‚úÖ Redimensionnement termin√© pour toutes les classes.\n")

"""2-2. üìè D√©coupage d‚Äôimages + Mise √† jour COCO .json"""

import json

input_json_path = "/content/drive/MyDrive/Colab_Notebooks/new_data/annotation_json/annotations_coco_new.json"
output_json_path = "/content/drive/MyDrive/Colab_Notebooks/new_data/annotation_json/annotations_resized_new.json"

# Ratio de redimensionnement
original_size = (768, 768)
target_size = (256, 256)
scale_x = target_size[0] / original_size[0]
scale_y = target_size[1] / original_size[1]

# Charger le fichier JSON
with open(input_json_path, "r") as f:
    coco_data = json.load(f)

# Mettre √† jour les dimensions des images
for img in coco_data["images"]:
    img["width"] = target_size[0]
    img["height"] = target_size[1]

# Mettre √† jour les annotations (segmentation + bbox + area)
for ann in coco_data["annotations"]:
    # Redimensionnement de la segmentation
    if isinstance(ann["segmentation"], list):
        new_segmentation = []
        for seg in ann["segmentation"]:
            new_seg = []
            for i in range(0, len(seg), 2):
                x = seg[i] * scale_x
                y = seg[i+1] * scale_y
                new_seg.extend([x, y])
            new_segmentation.append(new_seg)
        ann["segmentation"] = new_segmentation

    # Redimensionner bbox: [x, y, width, height]
    x, y, w, h = ann["bbox"]
    x *= scale_x
    y *= scale_y
    w *= scale_x
    h *= scale_y
    ann["bbox"] = [x, y, w, h]

    # Redimensionner l'aire
    ann["area"] *= scale_x * scale_y


# Sauvegarder le nouveau JSON redimensionn√©
with open(output_json_path, "w") as f:
    json.dump(coco_data, f, indent=2)

output_json_path

print("‚úÖ Fichier JSON redimensionn√© √† 128x128 et sauvegard√© avec succ√®s.")

import json
import os

json_path = "/content/drive/MyDrive/Colab_Notebooks/new_data/annotation_json/annotations_resized_new.json"

with open(json_path, 'r') as f:
    data = json.load(f)

# Nettoyage des chemins file_name
for img in data['images']:
    # Extraire uniquement le nom de classe et le nom de fichier
    parts = img["file_name"].split("/")
    if len(parts) >= 3:
        class_name = parts[-2]  # e.g. "aggregation"
        file_name = parts[-1]   # e.g. "MCF10A_....png"
        img["file_name"] = f"{class_name}/{file_name}"

# Sauvegarde dans un nouveau fichier (ou √©crase l'ancien si tu veux)
fixed_json_path = "/content/drive/MyDrive/Colab_Notebooks/new_data/annotation_json/annotations_resized_fixed_new.json"
with open(fixed_json_path, 'w') as f:
    json.dump(data, f, indent=2)

print("‚úÖ Chemins 'file_name' corrig√©s dans le JSON.")

"""le redimensionnement, la conversion des masques, puis le split en ensembles d‚Äôentra√Ænement, validation et test.

Objectif :
Adapter toutes les images et masques √† une taille unique pour l‚Äôentra√Ænement du mod√®le.

2-3. ‚úÇÔ∏è Split des donn√©es en Entra√Ænement / Validation / Test
"""

import os
import shutil
import json
import random
from collections import defaultdict

# === CONFIGURATION ===
ROOT_DIR = "/content/drive/MyDrive/Colab_Notebooks/new_data"
SRC_JSON = os.path.join(ROOT_DIR, "annotation_json/annotations_resized_fixed_new.json")
OUTPUT_DIR = os.path.join(ROOT_DIR, "split_data")

SRC_IMAGE_BASE = os.path.join(ROOT_DIR, "ResizedImgs")
SRC_MASK_BASE = os.path.join(ROOT_DIR, "ResizedMasks")

CLASSES = ["aggregation", "monocouche", "spheroide"]
SPLITS = ["train", "val", "test"]
SPLIT_RATIOS = [0.7, 0.15, 0.15]
RANDOM_SEED = 42

# === SETUP DES DOSSIERS ===
random.seed(RANDOM_SEED)
os.makedirs(os.path.join(OUTPUT_DIR, "annotations"), exist_ok=True)
for split in SPLITS:
    for kind in ["images", "masks"]:
        for cls in CLASSES:
            os.makedirs(os.path.join(OUTPUT_DIR, split, kind, cls), exist_ok=True)

# === LECTURE DU JSON COCO ===
with open(SRC_JSON, "r") as f:
    coco = json.load(f)

images_by_cls = defaultdict(list)
images_dict = {}
for img in coco["images"]:
    images_dict[img["id"]] = img
    for cls in CLASSES:
        if cls in img["file_name"].lower():
            images_by_cls[cls].append(img)
            break

# === REGROUPER ANNOTATIONS PAR IMAGE ===
annotations_by_image = defaultdict(list)
for ann in coco["annotations"]:
    annotations_by_image[ann["image_id"]].append(ann)

# === SPLIT + COPIE ===
split_images = {split: [] for split in SPLITS}
split_annotations = {split: [] for split in SPLITS}

for cls in CLASSES:
    imgs = images_by_cls[cls]
    random.shuffle(imgs)

    n = len(imgs)
    n_train = int(n * SPLIT_RATIOS[0])
    n_val = int(n * SPLIT_RATIOS[1])
    n_test = n - n_train - n_val

    split_map = {
        "train": imgs[:n_train],
        "val": imgs[n_train:n_train + n_val],
        "test": imgs[n_train + n_val:]
    }

    for split, split_imgs in split_map.items():
        for img in split_imgs:
            split_images[split].append(img)
            split_annotations[split].extend(annotations_by_image[img["id"]])

            # --- Correction des chemins ---
            rel_path = img["file_name"]  # ex: Images_converted/MCF10A-3000Cells/aggregation/...
            mask_rel_path = rel_path.replace("Images_converted", "ManualSegmentations_converted")

            src_img = os.path.join(SRC_IMAGE_BASE, rel_path)
            src_msk = os.path.join(SRC_MASK_BASE, mask_rel_path)

            dst_img = os.path.join(OUTPUT_DIR, split, "images", cls, os.path.basename(rel_path))
            dst_msk = os.path.join(OUTPUT_DIR, split, "masks", cls, os.path.basename(rel_path))

            # --- Copie conditionnelle ---
            if os.path.exists(src_img):
                shutil.copyfile(src_img, dst_img)
            else:
                print(f"‚ö†Ô∏è Image manquante : {src_img}")
            if os.path.exists(src_msk):
                shutil.copyfile(src_msk, dst_msk)
            else:
                print(f"‚ö†Ô∏è Masque manquant : {src_msk}")

# === SAUVEGARDE DES FICHIERS JSON SPLIT ===
for split in SPLITS:
    split_json = {
        "images": split_images[split],
        "annotations": split_annotations[split],
        "categories": coco["categories"]
    }
    out_json_path = os.path.join(OUTPUT_DIR, "annotations", f"{split}.json")
    with open(out_json_path, "w") as f:
        json.dump(split_json, f, indent=2)

print("\n‚úÖ Split termin√© avec structure images/masks coh√©rente.")

print("\nüìä Statistiques de r√©partition des images et annotations :\n")

stats = []

for split in SPLITS:
    split_image_list = split_images[split]
    split_ann_list = split_annotations[split]

    print(f"üîπ {split.upper()} - Total images : {len(split_image_list)}")
    print(f"   ‚Ä¢ Total annotations : {len(split_ann_list)}")

    # Comptage par classe
    count_by_class = defaultdict(int)
    for img in split_image_list:
        for cls in CLASSES:
            if cls in img["file_name"].lower():
                count_by_class[cls] += 1
                break

    for cls in CLASSES:
        print(f"   ‚Ä¢ {cls:<12}: {count_by_class[cls]} image(s)")

    stats.append({
        "split": split,
        "total_images": len(split_image_list),
        "total_annotations": len(split_ann_list),
        **{f"{cls}_images": count_by_class[cls] for cls in CLASSES}
    })

    print()

# ‚úÖ Enregistrement dans un CSV (optionnel)
import pandas as pd
df_stats = pd.DataFrame(stats)
stats_csv_path = os.path.join(OUTPUT_DIR, "split_statistics.csv")
df_stats.to_csv(stats_csv_path, index=False)
print(f"üìÅ Statistiques sauvegard√©es dans : {stats_csv_path}")

print("\nüìä Statistiques de r√©partition des images :\n")

for split in SPLITS:
    total_split = len(split_images[split])
    print(f"üîπ {split.upper()} - Total images : {total_split}")
    count_by_class = defaultdict(int)
    for img in split_images[split]:
        for cls in CLASSES:
            if cls in img["file_name"].lower():
                count_by_class[cls] += 1
                break
    for cls in CLASSES:
        print(f"   ‚Ä¢ {cls:<12}: {count_by_class[cls]}")
    print()

"""Divise les donn√©es pr√©trait√©es en jeux d'entra√Ænement, de validation et de test pour l'apprentissage et l'√©valuation des mod√®les.

7-Code pour splitter le fichier COCO .json

**üß† Partie 3 ‚Äî Augmentation, Dataset et DataLoader**

üß™ **1. Augmentation des Donn√©es avec albumentations**
L'augmentation permet de g√©n√©rer plus de diversit√© √† partir d'un petit jeu de donn√©es et am√©liore la robustesse du mod√®le en simulant des variations naturelles.
"""

!pip install pytorch_lightning

# Augmentation des images dans la phase d'entrainement
import albumentations as A
def get_training_augmentation():
    return A.Compose([
        A.HorizontalFlip(p=0.5),   # Retournement horizontal
        A.VerticalFlip(p=0.5),     # Retournement vertical
        A.RandomRotate90(p=0.5),
        A.Transpose(p=0.5),        # Transposition (diagonale)
        A.Affine(scale=(0.9, 1.1),rotate=(-45, 45),translate_percent=(0.0625, 0.0625),p=0.5 ),

        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ], p=1.0)
def get_validation_transforms():
    return A.Compose([

        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ], p=1.0)

def normalize_image(image_array):
    """Normalise une image en valeurs [0,1]."""
    return image_array.astype(np.float32) / 255.0

def one_hot_encode_mask(mask_array, num_classes):
    """Transforme un masque (valeurs discr√®tes) en repr√©sentation One-Hot."""
    mask_array = mask_array.astype(np.uint8)
    one_hot_mask = np.zeros((*mask_array.shape, num_classes), dtype=np.float32)
    for c in range(num_classes):
        one_hot_mask[:, :, c] = (mask_array == c).astype(np.float32)
    return one_hot_mask

"""Objectif :
Cr√©er une classe CellSegmentationDataset capable de charger
les images et masques selon 3 types :
*  mask_rcnn ‚Üí pour Mask R-CNN (instances + bounding boxes)
* multiclass ‚Üí pour Unet++ √† 3 classes avec softmax
  
*  binary_spheroid ‚Üí pour Unet++ binaire (sigmoid) sur les sph√©ro√Ødes

1. Cr√©er un Dataset PyTorch qui lit les annotations COCO
"""

from torch.utils.data import Dataset
from pycocotools.coco import COCO
import torchvision.transforms.functional as TF
import torch
import cv2
import numpy as np
from pathlib import Path

class CocoInstanceDataset(Dataset):
    def __init__(self, json_path, image_root_dir, transforms=None):
        self.coco = COCO(json_path)
        self.image_ids = self.coco.getImgIds()
        self.image_root_dir = Path(image_root_dir)
        self.transforms = transforms

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        image_id = self.image_ids[idx]
        image_info = self.coco.loadImgs(image_id)[0]

        # === Extraire le chemin relatif
        relative_path = image_info['file_name'].replace("\\", "/")  # compatibilit√© Windows/Linux
        img_path = self.image_root_dir / relative_path

        # === V√©rifier et charger l'image
        if img_path.suffix.lower() != ".png":
            raise FileNotFoundError(f"‚ùå Format non support√© (attendu .png) : {img_path}")

        image = cv2.imread(str(img_path))
        if image is None:
            raise FileNotFoundError(f"‚ùå Image introuvable ou corrompue : {img_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # === Appliquer les transformations Albumentations avant to_tensor
        if self.transforms:
            transformed = self.transforms(image=image)
            image = transformed["image"]

        image_tensor = TF.to_tensor(image)

        # === Annotations COCO
        ann_ids = self.coco.getAnnIds(imgIds=image_id)
        anns = self.coco.loadAnns(ann_ids)

        boxes, labels, masks = [], [], []
        for ann in anns:
            x, y, w, h = ann['bbox']
            boxes.append([x, y, x + w, y + h])
            labels.append(ann['category_id'])
            masks.append(self.coco.annToMask(ann))

        # === Pr√©parer les cibles
        if len(boxes) == 0:
            target = {
                "boxes": torch.zeros((0, 4), dtype=torch.float32),
                "labels": torch.zeros((0,), dtype=torch.int64),
                "masks": torch.zeros((0, image.shape[0], image.shape[1]), dtype=torch.uint8),
                "image_id": torch.tensor([image_id]),
                "iscrowd": torch.zeros((0,), dtype=torch.int64),
                "area": torch.zeros((0,), dtype=torch.float32),
            }
        else:
            boxes = torch.tensor(boxes, dtype=torch.float32)
            labels = torch.tensor(labels, dtype=torch.int64)
            masks = torch.as_tensor(np.stack(masks), dtype=torch.uint8)  # ‚úÖ rapide et propre
            areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
            iscrowd = torch.tensor([ann.get("iscrowd", 0) for ann in anns], dtype=torch.int64)

            target = {
                "boxes": boxes,
                "labels": labels,
                "masks": masks,
                "image_id": torch.tensor([image_id]),
                "iscrowd": iscrowd,
                "area": areas,
            }

        return image_tensor, target

"""* Forme Standard avec les masques PNG

üß† Partie 5 ‚Äî Entra√Ænement des mod√®les (U-Net++, Mask R-CNN)

5-1 Mod√©le MASK-RCNN

üîß 5-1-1. Importations des biblioth√®ques n√©cessaires
"""

!pip install segmentation_models_pytorch

!pip install --upgrade timm

!pip install segmentation_models_pytorch

"""üèóÔ∏è 5-1-2. Fonctions de cr√©ation des mod√®les

üìå Mask R-CNN avec adaptation des t√™tes de classification et de masquage
"""

import torch
import torchvision
from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor

def get_mask_rcnn_model(num_classes):
    # Charger le mod√®le avec poids pr√©-entra√Æn√©s
    weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT
    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=weights)

    # Adapter la t√™te de classification (bounding box)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    # Adapter la t√™te de pr√©diction de masque
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

    # üî• IMPORTANT : r√©duire les pr√©dictions par image pour √©viter l'OOM
    model.roi_heads.detections_per_img = 10  # par d√©faut = 100 ‚Üí trop pour Colab

    # (Optionnel) ajuster le NMS (Non-Max Suppression) pour encore r√©duire les pr√©dictions
    model.roi_heads.detections_per_img = 5  # üîΩ R√©duit √† 5
    model.rpn.nms_thresh = 0.4   # üîΩ Moins d'objets gard√©s
    model.rpn.pre_nms_top_n_train =   300         # R√©duction pendant entra√Ænement
    model.rpn.post_nms_top_n_train = 100
    model.rpn.pre_nms_top_n_val = 300         # R√©duction pendant entra√Ænement
    model.rpn.post_nms_top_n_val = 100
    model.rpn.pre_nms_top_n_test = 300         # R√©duction pendant entra√Ænement
    model.rpn.post_nms_top_n_test = 100

    return model

"""'''üü¢ Une architecture U-Net++ multiclasse (monocouche, agr√©gation) ‚Üí softmax

üü¢ Une architecture U-Net++ binaire (sph√©ro√Øde) ‚Üí sigmoid

üü¢ Un mod√®le Mask R-CNN pour l‚Äôinstance segmentation (surtout monocouche et agr√©gation)

‚úÖ Une strat√©gie de validation par Dice loss + IoU (Jaccard)

LightningModule pour Mask R-CNN
"""

import pytorch_lightning as pl
import torch
import torch.optim as optim

class MaskrcnnModel(pl.LightningModule):
    def __init__(self, model_name, num_classes, learning_rate, segmentation_type='mask_rcnn'):
        super().__init__()
        self.save_hyperparameters()
        self.num_classes = num_classes
        self.learning_rate = learning_rate
        self.segmentation_type = segmentation_type

        if model_name == "mask_rcnn":
            self.model = get_mask_rcnn_model(num_classes)
        else:
            raise ValueError(f"Unsupported model: {model_name}")

    def forward(self, images, targets=None):
        if targets is not None:
            return self.model(images, targets)
        else:
            return self.model(images)

    def training_step(self, batch, batch_idx):
      images, targets = batch
      loss_dict = self.model(images, targets)
      loss = sum(loss_dict.values())

      self.log("train_loss", loss)

      del images, targets, loss_dict
      torch.cuda.empty_cache()

      return loss


    def validation_step(self, batch, batch_idx):
        images, targets = batch

        # ‚ö†Ô∏è Mask R-CNN doit √™tre en mode train pour produire les losses
        self.model.train()
        with torch.no_grad():
            loss_dict = self.model(images, targets)
            loss = sum(loss_dict.values())

        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def configure_optimizers(self):
        return optim.Adam(self.parameters(), lr=self.learning_rate)

"""D√©finition de Dataset et DataLoader Pour Mask R-CNN"""

# === Pour am√©liorer la gestion m√©moire CUDA ===
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
torch.cuda.empty_cache()

from torch.utils.data import DataLoader, Subset

def collate_fn(batch):
    return tuple(zip(*batch))

# === Param√®tres JSON COCO ===
COCO_JSON_TRAIN = "/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/annotations/train.json"
COCO_JSON_VAL   = "/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/annotations/val.json"
COCO_JSON_TEST  = "/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/annotations/test.json"

# === Datasets originaux ===
train_dataset_full = CocoInstanceDataset(
    json_path=COCO_JSON_TRAIN,
    image_root_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/train/images",
    transforms=get_training_augmentation()
)

val_dataset_full = CocoInstanceDataset(
    json_path=COCO_JSON_VAL,
    image_root_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/val/images",
    transforms=get_validation_transforms()
)

test_dataset_full = CocoInstanceDataset(
    json_path=COCO_JSON_TEST,
    image_root_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/test/images",
    transforms=get_validation_transforms()
)

# === R√©duction √† 30 images max (√©vite crash GPU pendant debug)
MAX_SAMPLES = 30
train_dataset_maskrcnn = Subset(train_dataset_full, list(range(min(MAX_SAMPLES, len(train_dataset_full)))))
val_dataset_maskrcnn = Subset(val_dataset_full, list(range(min(8, len(val_dataset_full)))))
test_dataset_maskrcnn = Subset(test_dataset_full, list(range(min(8, len(test_dataset_full)))))


# === DataLoaders ===
train_loader_maskrcnn = DataLoader(
    train_dataset_maskrcnn,
    batch_size=1,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=0
)

val_loader_maskrcnn = DataLoader(
    val_dataset_maskrcnn,
    batch_size=1,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=0
)

test_loader_maskrcnn = DataLoader(
    test_dataset_maskrcnn,
    batch_size=1,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=0
)

print("Taille du train_loader:", len(train_loader_maskrcnn.dataset))  # doit afficher > 0
print("Taille du val_loader:", len(val_loader_maskrcnn.dataset))  # doit afficher > 0
print("Taille du test_loader:", len(test_loader_maskrcnn.dataset))  # doit afficher > 0

"""üîê 3. **Authentification**"""

from huggingface_hub import login
login(token="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")

# Commented out IPython magic to ensure Python compatibility.
torch.backends.cudnn.benchmark = True
# %env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

import gc
import torch
gc.collect()
torch.cuda.empty_cache()

"""Entrinement mod√®le Mask R-CNN"""

import os
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger


# === LOGGER ===
logger = TensorBoardLogger("lightning_logs", name="maskrcnn")

# === MODELE ===
model_maskrcnn = MaskrcnnModel(
    model_name="mask_rcnn",
    num_classes=NUM_CLASSES_ALL,
    learning_rate=LEARNING_RATE,
    segmentation_type="mask_rcnn"
)

# === CHECKPOINT ===
checkpoint_callback_maskrcnn = ModelCheckpoint(
    monitor="val_loss",
    mode="min",
    dirpath="/content/drive/MyDrive/checkpoints/maskrcnn",
    filename="best_maskrcnn",
    save_top_k=1,
    verbose=True
)

# === TRAINER CONFIGURATION ===
trainer_maskrcnn = pl.Trainer(
    max_epochs=MAX_EPOCHS,
    accelerator="auto",
    devices=1,
    precision="16-mixed",                # Mixed precision to reduce memory
    accumulate_grad_batches=2,
    callbacks=[checkpoint_callback_maskrcnn],
    logger=logger,
    log_every_n_steps=5,
    num_sanity_val_steps=0
)

# === ENTRA√éNEMENT ===
print("D√©marrage de l'entra√Ænement Mask R-CNN...")
trainer_maskrcnn.fit(model_maskrcnn, train_loader_maskrcnn, val_loader_maskrcnn)
torch.cuda.empty_cache()
print("‚úÖ Entra√Ænement Mask R-CNN termin√©.")

"""sauvegard√© le meilleur mod√©le"""

# Sauvegarder le mod√®le
model_maskrcnn = MaskrcnnModel.load_from_checkpoint(
    "drive/MyDrive/checkpoints/maskrcnn/best_maskrcnn.ckpt"
)
model_maskrcnn.eval().cuda()

"""Evolution de mod√©le dataset Validation maskrcnn"""

import pandas as pd
import numpy as np
import torch
from torchmetrics.classification import BinaryJaccardIndex, BinaryF1Score
from torch.utils.data import DataLoader

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def evaluate_maskrcnn_model(model, dataloader):
    model.eval().to(device)
    dice_metric = BinaryF1Score().to(device)
    iou_metric = BinaryJaccardIndex().to(device)
    dice_scores, iou_scores = [], []

    with torch.no_grad():
        for images, targets in dataloader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            outputs = model(images)

            for i in range(len(images)):
                if outputs[i]["masks"].shape[0] == 0:
                    continue

                pred_mask = torch.any(outputs[i]["masks"] > 0.3, dim=0).squeeze(0).float()
                true_mask = torch.any(targets[i]["masks"], dim=0).float()
                dice_scores.append(dice_metric(pred_mask.int(), true_mask.int()).item())
                iou_scores.append(iou_metric(pred_mask.int(), true_mask.int()).item())

    return round(np.mean(dice_scores), 4), round(np.mean(iou_scores), 4)


dice_maskrcnn, iou_maskrcnn= evaluate_maskrcnn_model(model_maskrcnn, val_loader_maskrcnn)
print(f"Dice score (Mask R-CNN):{dice_maskrcnn}")
print(f"Jaccad score (Mask R-CNN):{iou_maskrcnn}")

"""# üü¢ Une architecture U-Net++ multiclasse (monocouche, agr√©gation) ‚Üí softmax

# üü¢ Une architecture U-Net++ binaire (sph√©ro√Øde) ‚Üí sigmoid

# ‚úÖ Une strat√©gie de validation par Dice loss + IoU (Jaccard)

'''**üß† Partie 6 ‚Äî Mod√®les U-Net++ et Entra√Ænement avec PyTorch Lightning**

Nous allons d√©finir les deux mod√®les U-Net++ et leurs modules d'entra√Ænement.
'''
"""

import torch
import numpy as np
import cv2
import json
from torch.utils.data import Dataset
import torchvision.transforms.functional as TF
from pathlib import Path
from collections import defaultdict

class CellSegmentationDatasetFromJSON(Dataset):
    def __init__(self, json_path, images_dir, masks_dir, segmentation_type='multiclass', transform=None, image_size=(128, 128)):
        """
        Dataset utilisant un fichier COCO JSON et des masques PNG d√©j√† encod√©s en classes 1, 2, 3.

        Args:
            json_path (str): Chemin du fichier .json COCO.
            images_dir (str or Path): R√©pertoire contenant les images.
            masks_dir (str or Path): R√©pertoire contenant les masques PNG.
            segmentation_type (str): 'multiclass' ou 'binary'.
            transform (callable): Augmentations Albumentations.
            image_size (tuple): Taille cible des images.
        """
        self.json_path = json_path
        self.images_dir = Path(images_dir)
        self.masks_dir = Path(masks_dir)
        self.segmentation_type = segmentation_type
        self.transform = transform
        self.image_size = image_size
        self.spheroid_class_id = 3

        # Charger le JSON
        with open(json_path, 'r') as f:
            data = json.load(f)

        self.images = data["images"]
        self.image_id_to_filename = {img["id"]: img["file_name"] for img in self.images}

        # Pr√©parer mapping image_id ‚Üí chemin image/masque
        self.samples = []
        for img in self.images:
            cls = img["file_name"].split("/")[0]
            filename = img["file_name"].split("/")[-1]
            img_path = self.images_dir / cls / filename
            msk_path = self.masks_dir / cls / filename
            self.samples.append((img_path, msk_path))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, mask_path = self.samples[idx]

        # Chargement image
        image = cv2.imread(str(img_path))
        if image is None:
            raise FileNotFoundError(f"‚ùå Image non trouv√©e : {img_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Chargement masque
        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
        if mask is None:
            raise FileNotFoundError(f"‚ùå Masque non trouv√© : {mask_path}")

        # Redimensionnement
        if self.image_size:
            image = cv2.resize(image, self.image_size, interpolation=cv2.INTER_LINEAR)
            mask = cv2.resize(mask, self.image_size, interpolation=cv2.INTER_NEAREST)

        # Transformations (Albumentations)
        if self.transform:
            transformed = self.transform(image=image, mask=mask)
            image = transformed["image"]
            mask = transformed["mask"]

        image = TF.to_tensor(image)

        if self.segmentation_type == 'multiclass':
            multiclass_mask = np.zeros_like(mask)
            multiclass_mask[mask == 1] = 1  # monocouche
            multiclass_mask[mask == 2] = 2  # aggregation
            mask_tensor = torch.from_numpy(multiclass_mask).long()

        elif self.segmentation_type == 'binary':
            binary_mask = (mask == self.spheroid_class_id).astype(np.uint8)
            mask_tensor = torch.from_numpy(binary_mask).float()

        else:
            raise ValueError(f"Type de segmentation non support√© : {self.segmentation_type}")

        return image, mask_tensor

import timm
print("regnety_120" in timm.list_models())

"""D√©finition du Module PyTorch Lightning pour U-Net++ Multiclass"""

import torch
import pytorch_lightning as pl
import segmentation_models_pytorch as smp
from torchmetrics.classification import MulticlassJaccardIndex

class UnetMulticlassModel(pl.LightningModule):
    def __init__(self, num_classes, lr):
        super().__init__()

        self.model = smp.UnetPlusPlus(
            encoder_name="timm-regnety_120",
            encoder_weights="imagenet",
            in_channels=3,
            classes=num_classes,
            activation='softmax'  # U-Net++ sort directement des probabilit√©s
        )

        self.loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=False, ignore_index=0)
        self.iou_metric = MulticlassJaccardIndex(num_classes=num_classes, ignore_index=0)

        self.lr = lr
        self.save_hyperparameters()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        images, masks = batch

        # Assure que le masque est au format long (classe par pixel)
        if masks.ndim == 4 and masks.shape[1] == 1:
            masks = masks.squeeze(1)  # [B, 1, H, W] ‚Üí [B, H, W]
        masks = masks.long()

        probs = self(images)  # [B, C, H, W]
        loss = self.loss_fn(probs, masks)

        self.log('train_loss_multiclass', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        images, masks = batch

        if masks.ndim == 4 and masks.shape[1] == 1:
            masks = masks.squeeze(1)
        masks = masks.long()

        probs = self(images)
        loss = self.loss_fn(probs, masks)

        preds = torch.argmax(probs, dim=1)  # [B, H, W]
        iou = self.iou_metric(preds, masks)

        self.log('val_loss_multiclass', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log('val_iou', iou, on_step=False, on_epoch=True, prog_bar=True, logger=True)

        return {"val_loss": loss, "val_iou": iou}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.lr)

"""# --- D√©finition du Module PyTorch Lightning pour U-Net++ Binaire ---"""

import torch
import pytorch_lightning as pl
import segmentation_models_pytorch as smp
import torch.nn as nn
from torchmetrics.classification import BinaryJaccardIndex

class UnetBinaryModel(pl.LightningModule):
    def __init__(self, lr):
        super().__init__()

        # U-Net++ pour segmentation binaire
        self.model = smp.UnetPlusPlus(
            encoder_name="timm-regnety_120",
            encoder_weights="imagenet",
            in_channels=3,
            classes=1,                 # binaire = 1 canal
            activation='sigmoid'      # sigmoid pour sortie ‚àà [0,1]
        )

        # Fonction de perte combin√©e : BCE + Dice
        self.bce = nn.BCELoss()
        self.dice = smp.losses.DiceLoss(mode='binary')
        self.lr = lr

        # M√©trique IoU (Jaccard)
        self.iou_metric = BinaryJaccardIndex()
        self.save_hyperparameters()

    def forward(self, x):
        return self.model(x)  # sortie sigmoid√©e ‚àà [0,1]

    def training_step(self, batch, batch_idx):
        images, masks = batch  # masks : [B, 1, H, W] et float
        preds = self.forward(images)  # [B, 1, H, W]
        loss = self.bce(preds, masks.unsqueeze(1)) + self.dice(preds, masks.unsqueeze(1))

        self.log("train_loss_binary", loss, on_step=True, on_epoch=True, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
      images, masks = batch
      preds = self.forward(images)
      loss = self.bce(preds, masks.unsqueeze(1)) + self.dice(preds, masks.unsqueeze(1))

    # Seuillage pour IoU
      bin_preds = (preds > 0.3).float()
      iou = self.iou_metric(bin_preds.int(), masks.unsqueeze(1).int())

      self.log("val_loss_binary", loss, on_step=False, on_epoch=True, prog_bar=True)
      self.log("val_iou_binary", iou, on_step=False, on_epoch=True, prog_bar=True)


    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        return optimizer

"""**üß† Entra√Ænement U-Net++ Multiclasse (monocouche, agr√©gation)**

‚úÖ 1 Initialisation du mod√®le et de la callback

"""

from torch.utils.data import DataLoader

# === Dataset pour le mod√®le multiclasses (monocouche + agr√©gation) ===
train_dataset_multiclass = CellSegmentationDatasetFromJSON(
    json_path=COCO_JSON_TRAIN ,
    images_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/train/images",
    masks_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/train/masks",
    segmentation_type='multiclass',
    transform=get_training_augmentation(),
    image_size=(256, 256)
)

val_dataset_multiclass = CellSegmentationDatasetFromJSON(
    json_path=COCO_JSON_VAL,
    images_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/val/images",
    masks_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/val/masks",
    segmentation_type='multiclass',
    transform=get_validation_transforms(),
    image_size=(256, 256)
)
test_dataset_multiclass = CellSegmentationDatasetFromJSON(
    json_path=COCO_JSON_TEST,
    images_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/test/images",
    masks_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/test/masks",
    segmentation_type='multiclass',
    image_size=(256, 256)
)

# === R√©duction √† 30 images max (√©vite crash GPU pendant debug)
MAX_SAMPLES = 30

train_dataset_multiclass = Subset(train_dataset_multiclass, list(range(min(MAX_SAMPLES, len(train_dataset_multiclass)))))
val_dataset_multiclass  = Subset(val_dataset_multiclass, list(range(min(8, len(val_dataset_multiclass)))))
test_dataset_multiclass  = Subset(test_dataset_multiclass, list(range(min(8, len(test_dataset_multiclass)))))


# === DataLoaders ===
train_loader_multiclass = DataLoader(
    train_dataset_multiclass,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=2
)

val_loader_multiclass = DataLoader(
    val_dataset_multiclass,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2
)
print(f"\nNombre d'√©chantillons pour le mod√®le multiclasse (train): {len(train_dataset_multiclass)}")
print(f"Nombre d'√©chantillons pour le mod√®le multiclasse (val): {len(val_dataset_multiclass)}")

# --- Pr√©paration des DataLoaders pour les deux mod√®les ---
train_loader_multiclass = DataLoader(train_dataset_multiclass, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count())
val_loader_multiclass = DataLoader(val_dataset_multiclass, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count())

"""Entrainement UNET++ Multiclass"""

# --- Entra√Ænement du Mod√®le Multiclasse ---


print("\n--- Entra√Ænement du mod√®le U-Net++ Multiclasse (Monocouche, Agr√©gation) ---")
model_multiclass = UnetMulticlassModel(
    num_classes=NUM_CLASSES_UNET_MULTICLASS,
    lr=LEARNING_RATE
)

# Configuration du checkpoint automatique
checkpoint_callback_multiclass = ModelCheckpoint(
    monitor='val_loss_multiclass',                  # m√©trique √† suivre
    dirpath='drive/MyDrive/checkpoints/unetpp_multiclass',       # dossier de sortie
    filename='best_unet_multiclasses',              # nom du fichier .ckpt
    save_top_k=1,                                   # sauvegarde le meilleur mod√®le uniquement
    mode='min',                                     # on veut minimiser la loss
    verbose=True
)


trainer_multiclass = pl.Trainer(
    max_epochs=MAX_EPOCHS,
    callbacks=[checkpoint_callback_multiclass],
    accelerator='auto', # Utilise GPU si disponible, sinon CPU
    log_every_n_steps=1 # Pour voir le progr√®s plus souvent
)
trainer_multiclass.fit(model_multiclass, train_loader_multiclass, val_loader_multiclass)
print("\n‚úÖ Entra√Ænement du mod√®le multiclasse termin√©.")
print("üì¶ Mod√®le sauvegard√© automatiquement dans :")
print("   .checkpoints/unetpp_multiclass/best_unet_multiclasses.ckpt")

"""Sauvegarde le Meillieur Mod√©le"""

# Sauvegarder le mod√®le
model_multiclass = UnetMulticlassModel.load_from_checkpoint(
    "drive/MyDrive/checkpoints/unetpp_multiclass/best_unet_multiclasses.ckpt"
)

model_multiclass.eval().cuda()

from torchmetrics.classification import MulticlassJaccardIndex, MulticlassF1Score

def evaluate_multiclass_model(model, dataloader, num_classes=3):
    model.eval().to(device)

    iou_metric = MulticlassJaccardIndex(num_classes=num_classes, average="macro").to(device)
    dice_metric = MulticlassF1Score(num_classes=num_classes, average="macro").to(device)

    iou_scores, dice_scores = [], []

    with torch.no_grad():
        for images, masks in dataloader:
            images = images.to(device)
            masks = masks.to(device)

            preds = model(images)                     # [B, C, H, W]
            preds_class = preds.argmax(dim=1)         # [B, H, W]

            iou_scores.append(iou_metric(preds_class, masks).item())
            dice_scores.append(dice_metric(preds_class, masks).item())

    return round(np.mean(dice_scores), 4), round(np.mean(iou_scores), 4)

# === Utilisation ===
dice_unet_multi, iou_unet_multi = evaluate_multiclass_model(model_multiclass, val_loader_multiclass, num_classes=3)
print(f"Dice score (U-Net++): {dice_unet_multi}")
print(f"Jaccard score (U-Net++): {iou_unet_multi}")

"""**üß† 6. Entra√Ænement U-Net++ Binaire (sph√©ro√Øde uniquement)**

‚úÖ Initialisation du mod√®le


"""

# --- U-Net++ Binaire (sph√©ro√Ødes uniquement, classe id=3) ---
train_dataset_binaire = CellSegmentationDatasetFromJSON(
    json_path=COCO_JSON_TRAIN,
    images_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/train/images",
    masks_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/train/masks",
    segmentation_type='binary',  # ‚úÖ masque binaire uniquement pour classe spheroide (id=3)
    transform=get_training_augmentation(),
    image_size=(256,256)
)

val_dataset_binaire = CellSegmentationDatasetFromJSON(
    json_path=COCO_JSON_VAL,
    images_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/val/images",
    masks_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/val/masks",
    segmentation_type='binary',
    transform=get_validation_transforms(),
    image_size=(256,256)
)
test_dataset_binaire = CellSegmentationDatasetFromJSON(
    json_path=COCO_JSON_TEST,
    images_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/test/images",
    masks_dir="/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/test/masks",
    segmentation_type='binary',
    image_size=(256,256)

)

# === R√©duction √† 30 images max (√©vite crash GPU pendant debug)
MAX_SAMPLES = 30

train_dataset_binaire = Subset(train_dataset_binaire, list(range(min(MAX_SAMPLES, len(train_dataset_binaire)))))
val_dataset_binaire = Subset(val_dataset_binaire, list(range(min(8, len(val_dataset_binaire)))))
test_dataset_binaire  = Subset(test_dataset_binaire, list(range(min(8, len(test_dataset_binaire)))))

train_loader_binaire = DataLoader(
    train_dataset_binaire, batch_size=BATCH_SIZE, shuffle=True, num_workers=0
)

val_loader_binaire = DataLoader(
    val_dataset_binaire, batch_size=BATCH_SIZE, shuffle=False, num_workers=0
)

print(f"Nombre d'√©chantillons (train) : {len(train_dataset_binaire)}")
print(f"Nombre d'√©chantillons (val)   : {len(val_dataset_binaire)}")

""" --- Entra√Ænement du Mod√®le Binaire ---






"""

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint

# --- Entra√Ænement du Mod√®le Binaire ---
print("\n--- Entra√Ænement du mod√®le U-Net++ Binaire (Sph√©ro√Ødes) ---")

# 1. Cr√©ation du mod√®le
model_binary = UnetBinaryModel(lr=LEARNING_RATE)

# 2. Configuration du callback de sauvegarde automatique
checkpoint_callback_binary = ModelCheckpoint(
    monitor="val_loss_binary",
    mode="min",
    dirpath="drive/MyDrive/Colab_Notebooks/checkpoints/unetpp_binary",
    filename="best_unetpp_binary",
    save_top_k=1,
    verbose=True
)

# 3. Trainer
trainer_binaire = pl.Trainer(
    max_epochs=MAX_EPOCHS,
    callbacks=[checkpoint_callback_binary],
    accelerator='auto',
    log_every_n_steps=1
)

# 4. Entra√Ænement
trainer_binaire.fit(model_binary, train_loader_binaire, val_loader_binaire)

print("\n‚úÖ Entra√Ænement du mod√®le binaire termin√©.")
print("üì¶ Mod√®le sauvegard√© automatiquement dans :")
print("   .checkpoints/unetpp_binary/best_unetpp_binary.ckpt")

"""üíæ Sauvegarde

'''1. Charger le meilleur mod√®le apr√®s entra√Ænement
Apr√®s entra√Ænement, PyTorch Lightning sauvegarde le chemin du meilleur mod√®le dans :
'''
"""

model_binary = UnetBinaryModel.load_from_checkpoint(
    "drive/MyDrive/Colab_Notebooks/checkpoints/unetpp_binary/best_unetpp_binary.ckpt"
)
model_binary.eval().cuda()

# Commented out IPython magic to ensure Python compatibility.
# ‚úÖ Lance TensorBoard dans un notebook
# %load_ext tensorboard
# %tensorboard --logdir lightning_logs

"""üîÅ FONCTIONS D'√âVALUATION Unet++ binaire sur validation Dataset"""

from torchmetrics.classification import BinaryJaccardIndex, BinaryF1Score
import numpy as np
import torch
import matplotlib.pyplot as plt

def evaluate_binary_model(model, dataloader, threshold=0.05, verbose=True, visualize=True):
    model.eval().to(device)

    iou_metric = BinaryJaccardIndex().to(device)
    dice_metric = BinaryF1Score().to(device)

    iou_scores, dice_scores = [], []

    with torch.no_grad():
        for idx, (images, masks) in enumerate(dataloader):
            images = images.to(device)
            masks = masks.to(device)

            preds = model(images)  # [B, 1, H, W]
            preds_bin = (preds > threshold).float()  # Binarisation

            # V√©rification formes
            if masks.ndim == 3:
                masks = masks.unsqueeze(1)  # [B, 1, H, W]

            # üîç V√©rification : unique values
            if verbose:
                print(f"Image {idx + 1}")
                print("  ‚Üí mask unique:", torch.unique(masks))
                print("  ‚Üí preds unique:", torch.unique(preds_bin))

            # üî¨ Visualisation facultative
            if visualize and idx < 3:
                plt.figure(figsize=(10, 3))
                plt.subplot(1, 3, 1)
                plt.imshow(images[0].permute(1, 2, 0).cpu().numpy())
                plt.title("Image")

                plt.subplot(1, 3, 2)
                plt.imshow(masks[0][0].cpu(), cmap='gray')
                plt.title("Mask GT")

                plt.subplot(1, 3, 3)
                plt.imshow(preds_bin[0][0].cpu(), cmap='gray')
                plt.title("Prediction")
                plt.show()

            # V√©rification que GT contient des pixels positifs
            if masks.sum() == 0 and preds_bin.sum() == 0:
                continue  # Ignorer si rien dans le GT et rien d√©tect√©

            # Calcul m√©triques
            dice = dice_metric(preds_bin.int(), masks.int()).item()
            iou = iou_metric(preds_bin.int(), masks.int()).item()
            dice_scores.append(dice)
            iou_scores.append(iou)

    # Si aucune image valide, √©viter division par z√©ro
    if not dice_scores:
        print("‚ùå Aucune image avec objets d√©tect√©s / ground truth valides.")
        return 0.0, 0.0

    return round(np.mean(dice_scores), 4), round(np.mean(iou_scores), 4)

dice_unet, iou_unet = evaluate_binary_model(model_binary, val_loader_binaire)

print(f"\n‚úÖ Dice score (U-Net++ binaire): {dice_unet}")
print(f"‚úÖ Jaccard score (U-Net++ binaire): {iou_unet}")

"""**post-traitemen**

**texte en gras**""**üß† Partie 8 ‚Äî Inf√©rence et Combinaison des Models Unet++Multiclass et Unet++ bianary**

Apr√®s l'entra√Ænement, vous devrez charger les meilleurs mod√®les et combiner leurs pr√©dictions.

1 :Fusion des pr√©dictions et √©valuation finale
1. Pr√©paration des donn√©es de test (8 images)
"""

!pip install rglob

import os
import cv2
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import torch.nn.functional as F
import matplotlib.pyplot as plt

# === PARAM√àTRES G√âN√âRAUX ===
BINARY_THRESHOLD = 0.3


test_dataset_binaire = Subset(test_dataset_binaire, list(range(8)))
test_loader_binaire = DataLoader(test_dataset_binaire, batch_size=1, shuffle=False)

# === 1. Chargement des mod√®les U-Net++ entra√Æn√©s ===
import torch
from pathlib import Path

model_multiclass = UnetMulticlassModel.load_from_checkpoint(
    "drive/MyDrive/checkpoints/unetpp_multiclass/best_unet_multiclasses.ckpt"
)
model_binary = UnetBinaryModel.load_from_checkpoint(
    "drive/MyDrive/Colab_Notebooks/checkpoints/unetpp_binary/best_unetpp_binary.ckpt"
)

model_multiclass.eval().cuda()
model_binary.eval().cuda()
print("‚úÖ Mod√®les U-Net++ charg√©s (multiclasse + binaire)")

"""üß† 2. Fonction de pr√©diction combin√©e (logique XOR)

"""

# === Normalisation identique √† l'entra√Ænement ===
def normalize_input(image):
    image = image.astype(np.float32) / 255.0
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    return (image - mean) / std

# === Fonction de pr√©diction combin√©e (U-Net++ Multiclasse + Binaire) ===
def predict_combined(image_tensor, model_multiclass, model_binary, binary_threshold=0.6):
    with torch.no_grad():
        pred_multi = model_multiclass(image_tensor.unsqueeze(0))
        pred_multi = F.softmax(pred_multi, dim=1)
        pred_multi_class = torch.argmax(pred_multi, dim=1).squeeze(0).cpu().numpy()

        pred_bin = model_binary(image_tensor.unsqueeze(0))
        pred_bin = torch.sigmoid(pred_bin).squeeze().cpu().numpy()
        pred_bin_mask = (pred_bin > binary_threshold).astype(np.uint8)

        # Fusion conditionnelle : sph√©ro√Ødes seulement si pas d√©tect√©s par multiclass
        final_mask = pred_multi_class.copy()
        final_mask[(pred_bin_mask == 1) & (pred_multi_class == 0)] = 3

        return final_mask, pred_multi_class, pred_bin_mask

"""Boucle de test + Affichage sur 8 images"""

import matplotlib.pyplot as plt

def test_unetpp_fusion(model_multiclass, model_binary, dataloader):
    model_multiclass.eval()
    model_binary.eval()

    for idx, (image, _) in enumerate(dataloader):
        image = image.squeeze(0).permute(1, 2, 0).numpy()
        norm_img = normalize_input(image)
        image_tensor = torch.tensor(norm_img).permute(2, 0, 1).float().cuda()

        fusion, pred_multi, pred_bin = predict_combined(image_tensor, model_multiclass, model_binary)

        # Affichage
        fig, axes = plt.subplots(1, 4, figsize=(20, 5))
        titles = ["Image", "Multiclasse", "Binaire (sph√©ro√Ødes)", "Fusion finale"]
        contents = [image, pred_multi, pred_bin, fusion]

        for ax, title, img in zip(axes, titles, contents):
            ax.imshow(img if img.ndim == 2 else img.astype(np.uint8))
            ax.set_title(title)
            ax.axis("off")

        plt.suptitle(f"Image Test {idx+1}")
        plt.tight_layout()
        plt.show()

# üîç Lancer le test sur 8 images
test_unetpp_fusion(model_multiclass, model_binary, test_loader_binaire)

"""Sauvegarder"""

from pathlib import Path
import os

SAVE_DIR = Path("/content/drive/MyDrive/Colab_Notebooks/results/final_masks_unetpp_fusion")
SAVE_DIR.mkdir(parents=True, exist_ok=True)
print(f"üìÇ Masques fusionn√©s seront sauvegard√©s dans : {SAVE_DIR}")

def predict_and_save_fusion(model_multiclass, model_binary, dataloader, save_dir):
    model_multiclass.eval()
    model_binary.eval()

    for idx, (image_tensor, _) in enumerate(tqdm(dataloader)):
        image_np = image_tensor.squeeze(0).permute(1, 2, 0).numpy()
        norm_img = normalize_input(image_np)
        input_tensor = torch.tensor(norm_img).permute(2, 0, 1).unsqueeze(0).float().cuda()

        with torch.no_grad():
            # Multiclasse
            out_multi = model_multiclass(input_tensor)
            probs_multi = F.softmax(out_multi, dim=1)
            pred_multi = torch.argmax(probs_multi, dim=1).squeeze(0).cpu().numpy()

            # Binaire
            out_bin = model_binary(input_tensor)
            pred_bin = torch.sigmoid(out_bin).squeeze().cpu().numpy()
            pred_bin_mask = (pred_bin > 0.3).astype(np.uint8)

            # Fusion : sph√©ro√Øde = classe 3 si absent dans multi
            fusion_mask = pred_multi.copy()
            fusion_mask[(pred_bin_mask == 1) & (pred_multi == 0)] = 3

        # üîΩ Sauvegarde
        fusion_uint8 = (fusion_mask * 60).astype(np.uint8)  # multiplier pour visualiser les classes
        filename = f"image_{idx+1:02d}_fusion_mask.png"
        cv2.imwrite(str(save_dir / filename), fusion_uint8)

    print(f"\n‚úÖ Tous les masques fusionn√©s ont √©t√© sauvegard√©s dans : {save_dir}")

"""3Ô∏è‚É£ üîÅ Lancement de la pr√©diction et sauvegarde"""

predict_and_save_fusion(
    model_multiclass=model_multiclass,
    model_binary=model_binary,
    dataloader=test_loader_binaire,
    save_dir=SAVE_DIR
)

"""**Partie 9-valuation du Mod√®le Fusionn√© U-Net++ sur le Jeu de Test **

 Objectif :
Comparer les masques pr√©dits fusionn√©s avec les masques Ground Truth du dataset test, en mesurant les performances globales.


‚úÖ √âvaluer seule U-Net++

‚úÖ Calculer la Dice par classe, l‚ÄôIoU (Jaccard), la F1-Score, l‚ÄôAccuracy, la Recall

‚úÖ Afficher une matrice de confusion

‚úÖ Sauvegarder les r√©sultats

1-Imports et fonctions
"""

import torch
import pytorch_lightning as pl
import os
import glob
import cv2
from tqdm import tqdm
import numpy as np
from pathlib import Path
import sys
import pandas as pd

from torchmetrics import JaccardIndex, Accuracy, F1Score, Recall, ConfusionMatrix

from torchmetrics.classification import BinaryJaccardIndex
from torchmetrics.classification import BinaryF1Score
from torchmetrics.classification import BinaryAccuracy
from torchmetrics.classification import BinaryRecall
from torchmetrics.classification import BinaryConfusionMatrix

"""1Ô∏è‚É£ üìÇ Chargement des masques test & pr√©dits"""

# R√©pertoires
GT_MASK_DIR = Path("/content/drive/MyDrive/Colab_Notebooks/new_data/split_data/test/masks")  # Ground truth
PRED_MASK_DIR = Path("/content/drive/MyDrive/Colab_Notebooks/results/final_masks_unetpp_fusion")  # Pr√©dictions fusionn√©es

# Param√®tres
NUM_CLASSES = 4  # 0: fond, 1: monocouche, 2: aggregation, 3: spheroide

"""2- Initialiser les m√©triques multiclasses"""

import torch
from torchmetrics.classification import MulticlassJaccardIndex, MulticlassF1Score, MulticlassAccuracy, MulticlassRecall, MulticlassConfusionMatrix

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# M√©triques
iou_metric = MulticlassJaccardIndex(num_classes=NUM_CLASSES, average="macro", ignore_index=0).to(device)
dice_metric = MulticlassF1Score(num_classes=NUM_CLASSES, average="macro", ignore_index=0).to(device)
recall_metric = MulticlassRecall(num_classes=NUM_CLASSES, average="macro", ignore_index=0).to(device)
acc_metric = MulticlassAccuracy(num_classes=NUM_CLASSES, average="macro", ignore_index=0).to(device)
f1_macro = MulticlassF1Score(num_classes=NUM_CLASSES, average="macro", ignore_index=0).to(device)
cm_metric = MulticlassConfusionMatrix(num_classes=NUM_CLASSES, normalize=None).to(device)

# Accumulateurs
iou_scores, dice_scores, recall_scores, acc_scores, f1_scores = [], [], [], [], []
global_cm = torch.zeros((NUM_CLASSES, NUM_CLASSES), dtype=torch.int64).to(device)

"""3Ô∏è‚É£ üîÅ Boucle d‚Äô√©valuation"""

!pip install torchmetrics

gt_mask_paths = list(GT_MASK_DIR.rglob("*.png"))

for idx, gt_path in enumerate(tqdm(gt_mask_paths)):
    filename = gt_path.name
    pred_path = PRED_MASK_DIR / filename.replace(".png", "_fusion_mask.png")

    if not pred_path.exists():
        print(f"‚ùå Masque pr√©dictif manquant pour : {filename}")
        continue

    # Chargement
    gt_mask = cv2.imread(str(gt_path), cv2.IMREAD_GRAYSCALE)
    pred_mask = cv2.imread(str(pred_path), cv2.IMREAD_GRAYSCALE)

    # Resize si n√©cessaire
    if pred_mask.shape != gt_mask.shape:
        pred_mask = cv2.resize(pred_mask, (gt_mask.shape[1], gt_mask.shape[0]), interpolation=cv2.INTER_NEAREST)

    # R√©cup√©ration des vraies classes (converties)
    gt_tensor = torch.tensor(gt_mask // 60, dtype=torch.long).to(device)
    pred_tensor = torch.tensor(pred_mask // 60, dtype=torch.long).to(device)

    # M√©triques
    iou_scores.append(iou_metric(pred_tensor, gt_tensor).item())
    dice_scores.append(dice_metric(pred_tensor, gt_tensor).item())
    recall_scores.append(recall_metric(pred_tensor, gt_tensor).item())
    acc_scores.append(acc_metric(pred_tensor, gt_tensor).item())
    f1_scores.append(f1_macro(pred_tensor, gt_tensor).item())
    global_cm += cm_metric(pred_tensor, gt_tensor)

"""4Ô∏è‚É£ üìä R√©sultats globaux"""

print("\nüéØ R√©sultats globaux ‚Äî Mod√®le U-Net++ fusionn√© (8 images) :\n")
print(f"Dice (macro)      : {round(np.mean(dice_scores), 4)}")
print(f"Jaccard (IoU)     : {round(np.mean(iou_scores), 4)}")
print(f"Recall (macro)    : {round(np.mean(recall_scores), 4)}")
print(f"F1-score (macro)  : {round(np.mean(f1_scores), 4)}")
print(f"Accuracy (macro)  : {round(np.mean(acc_scores), 4)}")

"""5Ô∏è‚É£ üîç Matrice de confusion"""

import matplotlib.pyplot as plt
import seaborn as sns

labels = ["Fond", "Monocouche", "Agr√©gation", "Sph√©ro√Øde"]
cm_np = global_cm.cpu().numpy()

plt.figure(figsize=(6, 5))
sns.heatmap(cm_np, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel("Pr√©dit")
plt.ylabel("V√©rit√© terrain")
plt.title("Matrice de confusion - U-Net++ fusionn√©")
plt.tight_layout()
plt.show()

""" R√©sum√© + sauvegarde CSV"""

results = []
gt_mask_paths = list(GT_MASK_DIR.rglob("*.png"))

for idx, gt_path in enumerate(tqdm(gt_mask_paths)):
    filename = gt_path.name
    pred_path = PRED_MASK_DIR / filename.replace(".png", "_fusion_mask.png")

    if not pred_path.exists():
        print(f"‚ùå Masque pr√©dictif manquant pour : {filename}")
        continue

    # Chargement
    gt_mask = cv2.imread(str(gt_path), cv2.IMREAD_GRAYSCALE)
    pred_mask = cv2.imread(str(pred_path), cv2.IMREAD_GRAYSCALE)

    if pred_mask.shape != gt_mask.shape:
        pred_mask = cv2.resize(pred_mask, (gt_mask.shape[1], gt_mask.shape[0]), interpolation=cv2.INTER_NEAREST)

    gt_tensor = torch.tensor(gt_mask // 60, dtype=torch.long).to(device)
    pred_tensor = torch.tensor(pred_mask // 60, dtype=torch.long).to(device)

    # M√©triques
    iou = iou_metric(pred_tensor, gt_tensor).item()
    dice = dice_metric(pred_tensor, gt_tensor).item()
    recall = recall_metric(pred_tensor, gt_tensor).item()
    acc = acc_metric(pred_tensor, gt_tensor).item()
    f1 = f1_macro(pred_tensor, gt_tensor).item()
    cm = cm_metric(pred_tensor, gt_tensor)
    global_cm += cm

    # Stockage ligne par image
    results.append({
        "Image": filename,
        "Dice": dice,
        "IoU": iou,
        "Recall": recall,
        "F1-score": f1,
        "Accuracy": acc
    })

"""4. Affichage & Sauvegarde"""

import pandas as pd

df_results = pd.DataFrame(results)
df_mean = df_results.drop(columns=["Image"]).mean().to_frame(name="U-Net++ Fusionn√©")
df_mean.index.name = "M√©trique"

print("\nüìä Moyennes globales :")
print(df_mean)

SAVE_DIR = Path("/content/drive/MyDrive/Results")
SAVE_DIR.mkdir(parents=True, exist_ok=True)

detailed_csv_path = SAVE_DIR / "fusion_unetpp_detailed_results.csv"
mean_csv_path = SAVE_DIR / "fusion_unetpp_mean_metrics.csv"

df_results.to_csv(detailed_csv_path, index=False)
df_mean.to_csv(mean_csv_path)

print(f"\n‚úÖ R√©sultats d√©taill√©s sauvegard√©s dans : {detailed_csv_path}")
print(f"‚úÖ Moyennes globales sauvegard√©es dans : {mean_csv_path}")

"""Partie 10:  Detection_Objets (MaskRCNN)

1 : IMPORT DES LIBRAIRIES
"""

import torch
import torchvision
import torchvision.transforms.functional as TF
from torchvision.models.detection import maskrcnn_resnet50_fpn
import numpy as np
import cv2
from pathlib import Path
from pycocotools.coco import COCO

"""2 : CONFIGURATION DES DOSSIERS DE SORTIE"""

from pycocotools.coco import COCO
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image
from pathlib import Path
import torch
import cv2
import torchvision.transforms.functional as TF

# === Configuration ===
COCO_JSON_PATH = "/content/drive/MyDrive/new_data/annotations_coco.json"
IMAGE_DIR = Path("/content/drive/MyDrive/new_data/Images_converted/MCF10A-3000Cells")
OUTPUT_DIR = Path("results/maskrcnn_detections")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SCORE_THRESHOLD = 0.3

# === Chargement fichier COCO
coco = COCO(COCO_JSON_PATH)
category_id_to_name = {cat["id"]: cat["name"] for cat in coco.loadCats(coco.getCatIds())}

"""3-fonction de visualisation"""

def visualize_and_save_detections(image_tensor, boxes, labels, scores, save_path, class_names, score_threshold=0.3):
    filtered = [(b, l, s) for b, l, s in zip(boxes, labels, scores) if s >= score_threshold]
    if not filtered:
        print(f"‚ùå Aucun objet d√©tect√© au-dessus du seuil pour {save_path.name}")
        return

    boxes = torch.stack([f[0] for f in filtered])
    labels = [f[1] for f in filtered]
    scores = [f[2] for f in filtered]
    label_texts = [f"{class_names[l.item()]}: {s:.2f}" for l, s in zip(labels, scores)]

    image_with_boxes = draw_bounding_boxes(
        (image_tensor * 255).byte().cpu(),
        boxes=boxes.cpu(),
        labels=label_texts,
        colors="red",
        width=2,
        font_size=16
    )

    to_pil_image(image_with_boxes).save(save_path)
    print(f"‚úÖ Image sauvegard√©e : {save_path.name}")

"""3. Boucle de d√©tection et sauvegarde"""

from torchvision.models.detection import maskrcnn_resnet50_fpn
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

def get_trained_maskrcnn_model(ckpt_path, num_classes=4):
    model = maskrcnn_resnet50_fpn(weights=None)
    in_features_box = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features_box, num_classes)
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)

    ckpt = torch.load(ckpt_path, map_location=DEVICE)
    state_dict = {k.replace("model.", ""): v for k, v in ckpt["state_dict"].items()}
    model.load_state_dict(state_dict, strict=False)
    return model.to(DEVICE).eval()

# === Charger le mod√®le
model = get_trained_maskrcnn_model("/content/checkpoints/maskrcnn/best_maskrcnn.ckpt", num_classes=4)

# === Parcourir toutes les images du JSON
for img_id in coco.getImgIds():
    img_info = coco.loadImgs(img_id)[0]
    img_path = IMAGE_DIR / Path(img_info["file_name"]).name

    image = cv2.imread(str(img_path))
    if image is None:
        print(f"‚ùå Image introuvable : {img_path.name}")
        continue

    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    tensor = TF.to_tensor(image_rgb).to(DEVICE)

    with torch.no_grad():
        output = model([tensor])[0]

    visualize_and_save_detections(
        image_tensor=tensor.cpu(),
        boxes=output["boxes"],
        labels=output["labels"],
        scores=output["scores"],
        save_path=OUTPUT_DIR / f"{img_path.stem}_detected.png",
        class_names=category_id_to_name,
        score_threshold=SCORE_THRESHOLD
    )

"""Inference + affichage + sauvegarde

Evaluation MaskRCNN

üìÇ Donn√©es attendues :

Masques pr√©dits par Mask R-CNN (binaires par instance ou fusionn√©s)

Masques de v√©rit√© terrain (GT binaires, 0 pour fond, 1 pour sph√©ro√Øde)
"""

# === √âvaluation
metric_iou = JaccardIndex(task='binary').to(DEVICE)
metric_dice = F1Score(task='binary').to(DEVICE)
metric_acc = Accuracy(task='binary').to(DEVICE)
metric_recall = Recall(task='binary').to(DEVICE)
metric_f1 = F1Score(task='binary').to(DEVICE)
metric_confmat = ConfusionMatrix(task="binary").to(DEVICE)

results = []
global_cm = torch.zeros((2, 2), dtype=torch.int64).to(DEVICE)

pred_files = sorted(PRED_MASK_DIR.glob("*_mask.png"))

for pred_path in tqdm(pred_files):
    name = pred_path.stem.replace("_maskrcnn", "")
    gt_path = GT_MASK_DIR / f"{name}.png"

    if not gt_path.exists():
        print(f"‚ùå GT manquant : {gt_path}")
        continue

    pred = cv2.imread(str(pred_path), cv2.IMREAD_GRAYSCALE)
    gt = cv2.imread(str(gt_path), cv2.IMREAD_GRAYSCALE)

    pred = cv2.resize(pred, IMAGE_SIZE)
    gt = cv2.resize(gt, IMAGE_SIZE)

    pred_tensor = torch.tensor((pred > 0).astype(np.uint8), dtype=torch.int32).to(DEVICE)
    gt_tensor = torch.tensor((gt > 0).astype(np.uint8), dtype=torch.int32).to(DEVICE)

    # M√©triques
    iou_val = metric_iou(pred_tensor, gt_tensor).item()
    dice_val = metric_dice(pred_tensor, gt_tensor).item()
    acc_val = metric_acc(pred_tensor, gt_tensor).item()
    recall_val = metric_recall(pred_tensor, gt_tensor).item()
    f1_val = metric_f1(pred_tensor, gt_tensor).item()
    global_cm += metric_confmat(pred_tensor, gt_tensor)

    results.append({
        "image": name,
        "IoU": iou_val,
        "Dice": dice_val,
        "Accuracy": acc_val,
        "Recall": recall_val,
        "F1-Score": f1_val
    })

# === R√©sum√©
df = pd.DataFrame(results)
df.to_csv("results/maskrcnn_metrics.csv", index=False)

print("\nüìä Moyennes globales :")
print(df.mean(numeric_only=True))

# === Matrice de confusion
cm_np = global_cm.cpu().numpy()
labels = ["Fond", "Sph√©ro√Øde"]

plt.figure(figsize=(5, 4))
sns.heatmap(cm_np, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.title("Matrice de Confusion - Mask R-CNN")
plt.xlabel("Pr√©dit")
plt.ylabel("R√©el")
plt.tight_layout()
plt.savefig("results/maskrcnn_confusion_matrix.png")
plt.show()

"""1 : IMPORT DES LIBRAIRIES

"""

import torch
import pytorch_lightning as pl
import os
import glob
import cv2
import numpy as np
from pathlib import Path
import sys
import pandas as pd
import json  # Pour sauvegarder les r√©sultats mAP en JSON

# M√©triques de d√©tection d'objets
from torchmetrics.detection.mean_ap import MeanAveragePrecision
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks
from torch.utils.data import DataLoader

# Configuration des paths
project_root = Path(os.getcwd()).parent
sys.path.append(str(project_root))

"""2 : CONFIGURATION DES DOSSIERS DE SORTIE

"""

"""2 : CONFIGURATION DES DOSSIERS DE SORTIE"""

OUTPUT_DIR_OBJECT_DETECTION = "./output_object_detection"
os.makedirs(OUTPUT_DIR_OBJECT_DETECTION, exist_ok=True)
print(f"Dossier de sortie pour la d√©tection d'objets: {OUTPUT_DIR_OBJECT_DETECTION}")

"""3: PREPARATION DU DATALOADER POUR MASK R-CNN"""

def collate_fn(batch):
    """Fonction de collation personnalis√©e pour Mask R-CNN"""
    return tuple(zip(*batch))

# Cr√©ation du dataset et dataloader
test_dataset_maskrcnn = CellSegmentationDataset(
    X_test,
    M_test,
    num_classes=4,
    segmentation_type='mask_rcnn',
    transform=get_validation_transforms()
)

test_loader_maskrcnn = torch.utils.data.DataLoader(
    test_dataset_maskrcnn,
    batch_size=1,
    shuffle=False,
    num_workers=0,  # √âvite les probl√®mes de multiprocessing
    collate_fn=collate_fn
)

print(f"Images de test charg√©es pour Mask R-CNN: {len(X_test)}")

"""4 : CHARGEMENT DU MODELE MASK R-CNN"""

try:
    # Chargement du checkpoint
    model_maskrcnn = MaskrcnnModel.load_from_checkpoint(
        checkpoint_path="checkpoints/maskrcnn/best_maskrcnn.ckpt",
        model_name="mask_rcnn",
        num_classes=NUM_CLASSES_ALL,  # Classes + fond
        segmentation_type="mask_rcnn"
    )
    model_maskrcnn.eval()  # Mode √©valuation
    print("Mod√®le Mask R-CNN charg√© avec succ√®s.")
except Exception as e:
    print(f"Erreur lors du chargement du mod√®le Mask R-CNN: {e}")
    model_maskrcnn = None

"""3: PREPARATION DU DATALOADER POUR MASK R-CNN

"""

if model_maskrcnn:
    print("\n√âvaluation d√©taill√©e du mod√®le Mask R-CNN...")

    # Configuration du device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_maskrcnn.to(device)

    # Initialisation des m√©triques
    metric_map = MeanAveragePrecision(
        box_format="xyxy",
        iou_type="segm",
        class_metrics=True
    )
    metric_map.to(device)

    with torch.no_grad():  # D√©sactivation des gradients
        for i, (images, targets) in enumerate(test_loader_maskrcnn):
            # Transfert des donn√©es vers le device
            images = [img.to(device) for img in images]
            targets_on_device = [{k: v.to(device) for k, v in t.items()} for t in targets]

            # Pr√©diction du mod√®le
            outputs = model_maskrcnn(images)

            # Formatage des pr√©dictions et targets pour les m√©triques
            preds_formatted = []
            for out in outputs:
                masks = (out["masks"] > 0.5).squeeze(1) if "masks" in out else torch.empty(0, IMAGE_SIZE[0], IMAGE_SIZE[1], dtype=torch.bool, device=device)
                preds_formatted.append({
                    "boxes": out["boxes"],
                    "scores": out["scores"].float(),
                    "labels": out["labels"],
                    "masks": masks
                })

            targets_formatted = []
            for tgt in targets_on_device:
                masks = tgt["masks"].squeeze(1) if "masks" in tgt else torch.empty(0, IMAGE_SIZE[0], IMAGE_SIZE[1], dtype=torch.bool, device=device)
                targets_formatted.append({
                    "boxes": tgt["boxes"],
                    "labels": tgt["labels"],
                    "masks": masks
                })

            # Mise √† jour des m√©triques
            metric_map.update(preds_formatted, targets_formatted)

    #  AFFICHAGE DES RESULTATS

    final_map_results = metric_map.compute()

    print("\nR√©sultats d'√©valuation Mask R-CNN (mAP pour la segmentation):")
    print(f" mAP: {final_map_results['map'].item():.4f}")
    print(f" mAP_50: {final_map_results['map_50'].item():.4f}")
    print(f" mAP_75: {final_map_results['map_75'].item():.4f}")
    print(f" mAP_small: {final_map_results['map_small'].item():.4f}")
    print(f" mAP_medium: {final_map_results['map_medium'].item():.4f}")
    print(f" mAP_large: {final_map_results['map_large'].item():.4f}")
    print(f" Recall (AR@100): {final_map_results['mar_100'].item():.4f}")
    # M√©triques par classe
    map_per_class_data = {}
    # Correction de la correspondance des classes pour l'affichage
    class_labels_map = {
        1: CLASSES_DICT[1], # "aggregation"
        2: CLASSES_DICT[2], # "monocouche"
        3: CLASSES_DICT[3]  # "spheroide"
    }

    if 'map_per_class' in final_map_results and final_map_results['map_per_class'] is not None:
         print("\nmAP par classe (segmentation):")
         # Convertir le tensor en numpy array si n√©cessaire
         if torch.is_tensor(final_map_results['map_per_class']):
             map_per_class_values = final_map_results['map_per_class'].cpu().numpy()
         else:
             map_per_class_values = final_map_results['map_per_class']

         # V√©rifier si c'est un array 1D
         if isinstance(map_per_class_values, np.ndarray) and map_per_class_values.ndim == 1:
             # Les indices de map_per_class correspondent aux labels (1, 2, 3...)
             for i, map_val in enumerate(map_per_class_values):
                 # L'indice i ici est 0-based, mais nos labels sont 1-based (1, 2, 3)
                 # Donc, le label r√©el est i+1
                 class_name = class_labels_map.get(i + 1, f"Classe {i+1}")
                 print(f" {class_name}: {map_val:.4f}")
                 map_per_class_data[class_name] = float(map_val)
         else:
             print("Format de map_per_class non support√© ou vide.")
    else:
        print("Aucune m√©trique par classe disponible.")

"""6 : AFFICHAGE DES RESULTATS AVEC TABLEAU

**TEST**
"""

# ==============================================================
# üöÄ 1Ô∏è‚É£  IMPORTS ET CONFIG
# ==============================================================

import os
import sys
from pathlib import Path

import torch
import pytorch_lightning as pl
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from torchmetrics.detection.mean_ap import MeanAveragePrecision

# ‚û§ Tes modules perso (ajuste le chemin)
project_root = Path(os.getcwd()).parent
sys.path.append(str(project_root))

# Exemple : d√©commente et ajuste
# from model1 import CellSegmentationDataset, CellSegmentationModel, get_validation_transforms

# üìÅ Dossiers de sortie
OUTPUT_DIR = "./output_object_detection"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ==============================================================
# üöÄ 2Ô∏è‚É£  DEVICE + MODELE
# ==============================================================

# D√©finir le device d'abord !
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚úÖ Device utilis√© : {device}")

# Charger le mod√®le (ajuste le chemin du checkpoint et les param√®tres)
checkpoint_path = "checkpoints/maskrcnn/best_maskrcnn.ckpt"

if not os.path.exists(checkpoint_path):
    raise FileNotFoundError(f"‚ùå Checkpoint introuvable : {checkpoint_path}")

try:
    model_maskrcnn = MaskrcnnModel.load_from_checkpoint(
        checkpoint_path=checkpoint_path,
        model_name="mask_rcnn",
        num_classes=4,
        segmentation_type="mask_rcnn"
    )
    model_maskrcnn.to(device)
    model_maskrcnn.eval()
    print("‚úÖ Mod√®le Mask R-CNN charg√©.")
except Exception as e:
    raise RuntimeError(f"‚ùå Erreur lors du chargement : {e}")

# ==============================================================
# üöÄ 3Ô∏è‚É£  DATA LOADER (exemple ‚Äî adapte √† ton dataset)
# ==============================================================

def collate_fn(batch):
    """Fonction de collation Mask R-CNN"""
    return tuple(zip(*batch))

# D√©commente et adapte :
# test_dataset_maskrcnn = CellSegmentationDataset(X_test, M_test, num_classes=4, segmentation_type='mask_rcnn', transform=get_validation_transforms())
# test_loader_maskrcnn = torch.utils.data.DataLoader(test_dataset_maskrcnn, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)

print("‚úÖ Dataset et DataLoader configur√©s.")

# ==============================================================
# üöÄ 4Ô∏è‚É£  EVALUATION ET CALCUL mAP
# ==============================================================

metric_map = MeanAveragePrecision(box_format="xyxy", iou_type="segm", class_metrics=True)
metric_map.to(device)

print("‚è≥ √âvaluation en cours...")

with torch.no_grad():
    for images, targets in test_loader_maskrcnn:
        images = [img.to(device) for img in images]
        targets_on_device = [{k: v.to(device) for k, v in t.items()} for t in targets]

        outputs = model_maskrcnn(images)

        preds_formatted = []
        for out in outputs:
            masks = (out["masks"] > 0.5).squeeze(1) if "masks" in out else torch.empty(0, device=device)
            preds_formatted.append({
                "boxes": out["boxes"],
                "scores": out["scores"].float(),
                "labels": out["labels"],
                "masks": masks
            })

        targets_formatted = []
        for tgt in targets_on_device:
            masks = tgt["masks"].squeeze(1) if "masks" in tgt else torch.empty(0, device=device)
            targets_formatted.append({
                "boxes": tgt["boxes"],
                "labels": tgt["labels"],
                "masks": masks
            })

        metric_map.update(preds_formatted, targets_formatted)

final_map_results = metric_map.compute()

print("\n‚úÖ √âvaluation termin√©e.")
print(f"mAP : {final_map_results['map'].item():.4f}")

# ==============================================================
# üöÄ 5Ô∏è‚É£  METRIQUES PAR CLASSE
# ==============================================================

class_labels_map = {1: "monocouche", 2: "aggregation", 3: "spheroide"}

map_per_class_data = {}
if torch.is_tensor(final_map_results['map_per_class']):
    map_per_class_values = final_map_results['map_per_class'].cpu().numpy()
else:
    map_per_class_values = final_map_results['map_per_class']

if isinstance(map_per_class_values, np.ndarray) and map_per_class_values.ndim == 1:
    for i, map_val in enumerate(map_per_class_values):
        class_name = class_labels_map.get(i + 1, f"Classe {i+1}")
        print(f" {class_name}: {map_val:.4f}")
        map_per_class_data[class_name] = float(map_val)

# ==============================================================
# üöÄ 6Ô∏è‚É£  CREATION ET SAUVEGARDE DU TABLEAU ET GRAPHIQUE
# ==============================================================

main_metrics = pd.DataFrame({
    'Metric': [
        'mAP', 'mAP_50', 'mAP_75',
        'mAP_small', 'mAP_medium', 'mAP_large', 'Recall (AR@100)'
    ],
    'Value': [
        final_map_results['map'].item(),
        final_map_results['map_50'].item(),
        final_map_results['map_75'].item(),
        final_map_results['map_small'].item(),
        final_map_results['map_medium'].item(),
        final_map_results['map_large'].item(),
        final_map_results['mar_100'].item()
    ]
})

class_metrics = pd.DataFrame([
    {'Class': cls_name, 'mAP': mAP_val}
    for cls_name, mAP_val in map_per_class_data.items()
])

# ‚û§ Sauvegarde CSV
main_metrics.to_csv(os.path.join(OUTPUT_DIR, "main_metrics.csv"), index=False)
class_metrics.to_csv(os.path.join(OUTPUT_DIR, "class_metrics.csv"), index=False)

# ‚û§ Graphique
plt.figure(figsize=(10, 6))
bars = plt.bar(main_metrics['Metric'], main_metrics['Value'],
               color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',
                      '#9467bd', '#8c564b', '#e377c2'])

for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.3f}', ha='center', va='bottom')

plt.title('Performance du mod√®le Mask R-CNN', fontsize=14)
plt.ylabel('Score', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

plt.savefig(os.path.join(OUTPUT_DIR, 'metrics_plot.png'),
            dpi=300, bbox_inches='tight')
plt.show()

print(f"‚úÖ R√©sultats sauvegard√©s dans : {OUTPUT_DIR}")

import torch
import os
import matplotlib.pyplot as plt
from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks
from torchvision.transforms.functional import to_pil_image

def visualize_maskrcnn_predictions(
    model,
    test_loader,
    output_dir="/content/maskrcnn_visualizations",
    num_examples=5,
    confidence_threshold=0.5
):
    """
    Visualisation et sauvegarde des pr√©dictions Mask R-CNN.

    Args:
        model: mod√®le Mask R-CNN (d√©j√† sur device)
        test_loader: DataLoader des images de test
        output_dir: dossier pour sauvegarder les PNG
        num_examples: nombre d'exemples √† afficher
        confidence_threshold: seuil pour filtrer les pr√©dictions
    """
    device = next(model.parameters()).device

    # D√©finir noms et couleurs pour chaque classe
    CLASS_CONFIG = {
        1: {"name": "monocouche", "color": "red"},
        2: {"name": "aggregation", "color": "green"},
        3: {"name": "spheroide", "color": "blue"}
    }

    os.makedirs(output_dir, exist_ok=True)

    model.eval()

    with torch.no_grad():
        processed = 0

        for batch_idx, (images, _) in enumerate(test_loader):
            if processed >= num_examples:
                break

            # Envoyer sur le device
            images = [img.to(device) for img in images]
            outputs = model(images)

            for img_idx, (image, output) in enumerate(zip(images, outputs)):
                if processed >= num_examples:
                    break

                keep = output['scores'] > confidence_threshold
                boxes = output['boxes'][keep].cpu()
                labels = output['labels'][keep].cpu()
                scores = output['scores'][keep].cpu()
                masks = (output['masks'][keep].squeeze(1) > 0.5).cpu()

                # Annuler normalisation (si tes images sont [0,1] normalis√©es)
                img = image.detach().cpu().clone()
                if img.min() < 0:  # [-1,1]
                    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                    img = img * std + mean
                img = (img * 255).clamp(0, 255).byte()

                if boxes.nelement() > 0:
                    labels_text = [
                        f"{CLASS_CONFIG.get(l.item(), {'name': f'class{l}'})['name']} {s:.2f}"
                        for l, s in zip(labels, scores)
                    ]
                    colors = [
                        CLASS_CONFIG.get(l.item(), {'color': 'yellow'})['color']
                        for l in labels
                    ]

                    img_with_boxes = draw_bounding_boxes(
                        img,
                        boxes=boxes,
                        labels=labels_text,
                        colors=colors,
                        width=2,
                        font_size=12
                    )

                    img_with_masks = draw_segmentation_masks(
                        img_with_boxes,
                        masks=masks,
                        colors=colors,
                        alpha=0.4
                    )
                else:
                    img_with_masks = img

                # Convertir pour afficher avec matplotlib
                img_pil = to_pil_image(img_with_masks)

                # Sauvegarder
                out_path = os.path.join(output_dir, f"maskrcnn_pred_{processed}.png")
                img_pil.save(out_path)

                # Afficher
                plt.figure(figsize=(10, 8))
                plt.imshow(img_pil)
                plt.title(f"Pr√©diction {processed + 1} - Confiance > {confidence_threshold}")
                plt.axis("off")
                plt.show()

                print(f"‚úÖ Sauvegard√© : {out_path}")

                processed += 1

    print(f"\n‚úÖ {processed} visualisations cr√©√©es dans {output_dir}")

visualize_maskrcnn_predictions(
    model=model_maskrcnn,
    test_loader=test_loader_maskrcnn,
    output_dir="./maskrcnn_visualizations",
    num_examples=5,  # Change pour + ou -
    confidence_threshold=0.7
)

"""DICE and Jaccad MaskRCNN"""

!pip install --upgrade torchmetrics

import torchmetrics
print(torchmetrics.__version__)

from torchmetrics import F1Score, JaccardIndex

# ‚û§ Initialiser
dice_metric_maskrcnn = F1Score(task='binary').to(device)  # F1 = Dice pour binaire
jaccard_metric_maskrcnn = JaccardIndex(task='binary').to(device)

with torch.no_grad():
    for i, (images, targets) in enumerate(test_loader_maskrcnn):
        images = [img.to(device) for img in images]
        targets_on_device = [{k: v.to(device) for k, v in t.items()} for t in targets]

        outputs = model_maskrcnn(images)

        preds_formatted = []
        masks_preds = []
        masks_true = []

        for out, tgt in zip(outputs, targets_on_device):
            # Pour metric_map (d√©tection)
            masks = (out["masks"] > 0.5).squeeze(1)
            preds_formatted.append({
                "boxes": out["boxes"],
                "scores": out["scores"].float(),
                "labels": out["labels"],
                "masks": masks
            })

            # Pour Dice/IoU pixel
            if masks.numel() > 0:
                pred_mask = masks.sum(dim=0).clamp(0,1).unsqueeze(0).int()
            else:
                pred_mask = torch.zeros_like(tgt["masks"][0]).unsqueeze(0).int()

            true_mask = tgt["masks"].sum(dim=0).clamp(0,1).unsqueeze(0).int()

            dice_metric_maskrcnn.update(pred_mask, true_mask)
            jaccard_metric_maskrcnn.update(pred_mask, true_mask)

        targets_formatted = [
            {"boxes": tgt["boxes"], "labels": tgt["labels"], "masks": tgt["masks"].squeeze(1)}
            for tgt in targets_on_device
        ]
        metric_map.update(preds_formatted, targets_formatted)

# ‚û§ Apr√®s la boucle :
dice_maskrcnn = dice_metric_maskrcnn.compute().item()
jaccard_maskrcnn = jaccard_metric_maskrcnn.compute().item()

print(f"‚úÖ Mask R-CNN Dice : {dice_maskrcnn:.4f}")
print(f"‚úÖ Mask R-CNN IoU  : {jaccard_maskrcnn:.4f}")

"""les valeur de Dice et Jaccad (unet++ et Maskrcnn)"""

# ‚ö° Cast explicite en float pour √©viter l'erreur de format
dice_unetpp = float(dice_val)
jaccard_unetpp = float(iou_val)
dice_maskrcnn = float(dice_maskrcnn)
jaccard_maskrcnn = float(jaccard_maskrcnn)

# ‚úÖ Utiliser les versions cast√©es
df_comparaison = pd.DataFrame({
    "Model": ["Mask R-CNN", "U-Net++"],
    "Dice (F1 Score)": [dice_maskrcnn, dice_unetpp],
    "Jaccard (IoU)": [jaccard_maskrcnn, jaccard_unetpp
]})

# ‚úÖ Formater seulement les colonnes num√©riques
display(
    df_comparaison.style
    .format({
        "Dice (F1 Score)": "{:.5f}",
        "Jaccard (IoU)": "{:.5f}"
    })
    .set_caption("Comparaison des m√©triques")
)
# ‚úÖ Sauvegarder
SAVE_COMPARISON = os.path.join(SAVE_DIR, "comparison_maskrcnn_unetpp.csv")
df_comparaison.to_csv(SAVE_COMPARISON, index=False)
print(f"\n‚úÖ Tableau de comparaison sauvegard√© : {SAVE_COMPARISON}")

"""# Partie II Mask RCNN avec Luminance LAB"""

"""Pour entra√Æner ou valider avec des images en luminance LAB"""

train_dataset_maskrcnn_lab = CellSegmentationDataset(
     X_train, M_train, num_classes=NUM_CLASSES_ALL, segmentation_type='mask_rcnn',
     transform=get_training_augmentation(), use_lab_luminance=True
 )
val_dataset_maskrcnn_lab = CellSegmentationDataset(
     X_val, M_val, num_classes=NUM_CLASSES_ALL, segmentation_type='mask_rcnn',
     transform=get_validation_transforms(), use_lab_luminance=True )

"""DataLoader"""

train_loader_maskrcnn_lab = DataLoader(train_dataset_maskrcnn_lab, pin_memory=False, collate_fn=collate_fn, batch_size=BATCH_SIZE, persistent_workers=False,  num_workers=2)
val_loader_maskrcnn_lab = DataLoader(val_dataset_maskrcnn_lab, pin_memory=False, batch_size=BATCH_SIZE, collate_fn=collate_fn,persistent_workers=False,  num_workers=2)
print(f"\nNombre d'√©chantillons pour le mod√®le multiclasse (train): {len(train_loader_maskrcnn_lab)}")
print(f"Nombre d'√©chantillons pour le mod√®le multiclasse (val): {len(val_loader_maskrcnn_lab)}")

"""Entrianement de mod√©les MaskRCNN_Lab"""

model_maskrcnn_lab = MaskrcnnModel(
    model_name="mask_rcnn",
    num_classes=NUM_CLASSES_ALL,
    learning_rate=LEARNING_RATE,
    segmentation_type="mask_rcnn"
)

checkpoint_callback= ModelCheckpoint(
    monitor="val_loss", # Pour Mask R-CNN, nous monitorons la perte de validation
    mode="min",
    dirpath="checkpoints/maskrcnn_lab",
    filename="best_maskrcnn_lab",
    save_top_k=1,
    verbose=True
)

print("Mod√®le Mask R-CNN et callbacks initialis√©s.")


trainer_maskrcnn_lab = pl.Trainer(
    max_epochs=MAX_EPOCHS,
    accelerator="auto",  # Laisse PL choisir automatiquement GPU ou CPU
    devices=1,
    callbacks=[checkpoint_callback],
    log_every_n_steps=10
)

print("D√©marrage de l'entra√Ænement Mask R-CNN LAB...")
trainer_maskrcnn_lab.fit(model_maskrcnn_lab, train_loader_maskrcnn_lab, val_loader_maskrcnn_lab)
print("Entra√Ænement Mask R-CNN LAB termin√©.")

"""# Sauvegarder le mod√®le"""

# Sauvegarder le mod√®le
model_maskrcnn_lab= MaskrcnnModel.load_from_checkpoint(
    "checkpoints/maskrcnn_lab/best_maskrcnn_lab.ckpt"
)

model_maskrcnn_lab.eval().cuda()

"""**Unet++Multiclass_Lab**"""

train_dataset_multiclasse_lab = CellSegmentationDataset(
     X_train, M_train, num_classes=NUM_CLASSES_UNET_MULTICLASS, segmentation_type='multiclass',
     transform=get_training_augmentation(), use_lab_luminance=True
 )
val_dataset_multiclasse_lab = CellSegmentationDataset(
     X_val, M_val, num_classes=NUM_CLASSES_UNET_MULTICLASS, segmentation_type='multiclass',
     transform=get_validation_transforms(), use_lab_luminance=True )

train_loader_multiclasse_lab = DataLoader(train_dataset_multiclasse_lab, pin_memory=False, batch_size=BATCH_SIZE, persistent_workers=False,  num_workers=2)
val_loader_multiclasse_lab = DataLoader(val_dataset_multiclasse_lab, pin_memory=False, batch_size=BATCH_SIZE,persistent_workers=False,  num_workers=2)
print(f"\nNombre d'√©chantillons pour le mod√®le multiclasse LAB (train): {len(train_loader_multiclasse_lab)}")
print(f"Nombre d'√©chantillons pour le mod√®le multiclasse LAB (val): {len(val_loader_multiclasse_lab)}")

"""Entrianement de model Unet++ Multiclasse LAB"""
# --- Entra√Ænement du Mod√®le Multiclasse ---


print("\n--- Entra√Ænement du mod√®le U-Net++ Multiclasse LAB ")
model_multiclasse_lab = UnetMulticlassModel(
    num_classes=NUM_CLASSES_UNET_MULTICLASS,
    lr=LEARNING_RATE
)

# Configuration du checkpoint automatique
checkpoint_callback_multiclass = ModelCheckpoint(
    monitor='val_loss_multiclass',                  # m√©trique √† suivre
    dirpath='checkpoints/unetpp_multiclass_lab',       # dossier de sortie
    filename='best_unet_multiclasses_lab',              # nom du fichier .ckpt
    save_top_k=1,                                   # sauvegarde le meilleur mod√®le uniquement
    mode='min',                                     # on veut minimiser la loss
    verbose=True
)


trainer_multiclass_lab = pl.Trainer(
    max_epochs=MAX_EPOCHS,
    callbacks=[checkpoint_callback_multiclass],
    accelerator='auto', # Utilise GPU si disponible, sinon CPU
    log_every_n_steps=1 # Pour voir le progr√®s plus souvent
)
trainer_multiclass_lab.fit(model_multiclasse_lab, train_loader_multiclasse_lab,val_loader_multiclasse_lab)
print("\n‚úÖ Entra√Ænement du mod√®le multiclasse LAB termin√©.")
print("üì¶ Mod√®le sauvegard√© automatiquement dans :")
print("   .checkpoints/unetpp_multiclass/best_unet_multiclasses_lab.ckpt")

# Sauvegarder le mod√®le
model_multiclasse_lab = UnetMulticlassModel.load_from_checkpoint(
    "checkpoints/unetpp_multiclass_lab/best_unet_multiclasses_lab.ckpt"
)
model_multiclasse_lab.eval().cuda()

"""Unet++ Binary Lab"""

train_dataset_binary_lab = CellSegmentationDataset(
     X_train, M_train, num_classes=NUM_CLASSES_ALL, segmentation_type='binary_spheroid',
     transform=get_training_augmentation(), use_lab_luminance=True
 )
val_dataset_binary_lab = CellSegmentationDataset(
     X_val, M_val, num_classes=NUM_CLASSES_ALL, segmentation_type='binary_spheroid',
     transform=get_validation_transforms(), use_lab_luminance=True )

train_loader_binary_lab = DataLoader(train_dataset_binary_lab, pin_memory=False, batch_size=BATCH_SIZE, persistent_workers=False,  num_workers=2)
val_loader_binary_lab = DataLoader(val_dataset_binary_lab, pin_memory=False, batch_size=BATCH_SIZE,persistent_workers=False,  num_workers=2)
print(f"\nNombre d'√©chantillons pour le mod√®le binaire (train): {len(train_loader_binary_lab )}")
print(f"Nombre d'√©chantillons pour le mod√®le binaire (val): {len(val_loader_binary_lab)}")

model_biniare_lab = UnetBinaryModel(lr=LEARNING_RATE)

checkpoint_callback_binaire_lab= ModelCheckpoint(
    monitor="val_loss_binary", # Pour unet++muliclasse, nous monitorons la perte de validation
    mode="min",
    dirpath="checkpoints/binaire_lab",
    filename="best_binaire_lab",
    save_top_k=1,
    verbose=True
)

print("Mod√®le Unet++ Biniare et callbacks initialis√©s.")


trainer_biniare_lab = pl.Trainer(
    max_epochs=MAX_EPOCHS,
    accelerator="auto",  # Laisse PL choisir automatiquement GPU ou CPU
    devices=1,
    callbacks=[checkpoint_callback_binaire_lab],
    log_every_n_steps=10
)

print("D√©marrage de l'entra√Ænement Unet++ Biniare LAB...")
trainer_biniare_lab.fit(model_biniare_lab, train_loader_binary_lab, val_loader_binary_lab)
print("Entra√Ænement Unet++ Biniare LAB termin√©.")

# Sauvegarder le mod√®le
model_binaire_lab = UnetBinaryModel.load_from_checkpoint(
    "checkpoints/binaire_lab/best_binaire_lab.ckpt"
)
model_binaire_lab.eval().cuda()

"""Mod√©le Unet++ LAb"""

import os
import cv2
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import rglob
import torch.nn.functional as F
import matplotlib.pyplot as plt

# === PARAM√àTRES G√âN√âRAUX ===
BINARY_THRESHOLD = 0.3
IMAGE_SIZE = (256, 256)

# === Dossiers ===
TEST = Path("/content/drive/MyDrive/Colab Notebooks/Data/IncuCyteS3_subset/dataset_split/test/images")
OUTPUT_DIR_Lab = Path("results/Predict_masks_Lab")
OUTPUT_DIR_Lab.mkdir(parents=True, exist_ok=True)


# === R√©cup√©ration des images de test ===
image_files = list(TEST.rglob("*.png"))
print(f"üîç {len(image_files)} images .png trouv√©es dans : {TEST}")

# === Normalisation identique √† l'entra√Ænement ===
def normalize_input(image):
    image = image.astype(np.float32) / 255.0
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    return (image - mean) / std

# === Fonction de pr√©diction combin√©e (U-Net++ Multiclasse + Binaire) ===
def predict_combined_lab(image_tensor, model_multiclasse_lab, model_binaire_lab, binary_threshold=0.3):
    with torch.no_grad():
        pred_multi = model_multiclasse_lab(image_tensor.unsqueeze(0))
        pred_multi = F.softmax(pred_multi, dim=1)
        pred_multi_class = torch.argmax(pred_multi, dim=1).squeeze(0).cpu().numpy()

        pred_bin = model_binaire_lab(image_tensor.unsqueeze(0))
        pred_bin = torch.sigmoid(pred_bin).squeeze().cpu().numpy()
        pred_bin_mask = (pred_bin > binary_threshold).astype(np.uint8)

        # Fusion conditionnelle : sph√©ro√Ødes seulement si pas d√©tect√©s par multiclass
        final_mask = pred_multi_class.copy()
        final_mask[(pred_bin_mask == 1) & (pred_multi_class == 0)] = 3

        return final_mask, pred_multi_class, pred_bin_mask

# === Fonction d'affichage optionnelle ===
def show_result(image, pred_multi, pred_bin, final_mask):
    fig, axes = plt.subplots(1, 4, figsize=(20, 5))
    titles = ["Image", "Multiclasse (Mono/Agg)", "Binaire (Sph√©ro√Øde)", "Fusion finale (XOR)"]
    images = [image, pred_multi, pred_bin, final_mask]

    for ax, title, img in zip(axes, titles, images):
        ax.imshow(img if img.ndim == 2 else img.astype(np.uint8))
        ax.set_title(title)
        ax.axis("off")

    plt.tight_layout()
    plt.show()

# === Inf√©rence et sauvegarde des masques ===
for img_path in tqdm(image_files):
    # --- Chargement de l‚Äôimage ---
    image = cv2.imread(str(img_path))
    if image is None:
        print(f"‚ö†Ô∏è Image introuvable : {img_path}")
        continue

    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, IMAGE_SIZE)
    normalized = normalize_input(image)
    image_tensor = torch.from_numpy(normalized).permute(2, 0, 1).float().cuda()

    # --- Pr√©diction ---
    final_mask, _, _ = predict_combined_lab(image_tensor, model_multiclasse_lab, model_binaire_lab)

    # --- Sauvegarde ---
    output_name = img_path.stem + "_mask.png"
    output_path_lab = OUTPUT_DIR_Lab / output_name
    cv2.imwrite(str(output_path_lab), (final_mask * 60).astype(np.uint8))  # Pour visualisation

print(f"\n‚úÖ Tous les masques Lab  ont √©t√© sauvegard√©s dans : {OUTPUT_DIR_Lab}")

import torch
import cv2
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from torchmetrics import JaccardIndex, F1Score, Accuracy, Recall, ConfusionMatrix
import matplotlib.pyplot as plt
import seaborn as sns

# === Param√®tres ===
IMAGE_SIZE = (256, 256)
PRED_MASK_DIR = Path("/content/results/Predict_masks_Lab")             # Masques pr√©dits : image_X_mask.png
GT_MASK_DIR = Path("/content/results/all_gt_masks")     # Masques GT : m√™me nom (image_X_mask.png)
SAVE_CSV = "binary_metrics_results.csv"
SAVE_CM = "confusion_matrix_binary.png"

# === Chargement des fichiers pr√©dits
pred_files = sorted(PRED_MASK_DIR.glob("*_mask.png"))
print(f"üîç {len(pred_files)} masques pr√©dits d√©tect√©s dans : {PRED_MASK_DIR}")

# === Initialisation des m√©triques
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
iou= JaccardIndex(task='binary').to(device)
dice= F1Score(task='binary').to(device)
acc = Accuracy(task='binary').to(device)
recall = Recall(task='binary').to(device)
conf_matrix = ConfusionMatrix(task='binary').to(device)

results = []
global_cm = torch.zeros((2, 2), dtype=torch.int64).to(device)

# === Boucle d'√©valuation
for pred_path in tqdm(pred_files):
    name = pred_path.name  # ex: image_01_mask.png
    gt_path = GT_MASK_DIR / name

    if not gt_path.exists():
        print(f"‚ùå Masque GT manquant : {gt_path.name}")
        continue

    # === Chargement des masques
    pred = cv2.imread(str(pred_path), cv2.IMREAD_GRAYSCALE)
    gt = cv2.imread(str(gt_path), cv2.IMREAD_GRAYSCALE)

    # Resize
    pred = cv2.resize(pred, IMAGE_SIZE, interpolation=cv2.INTER_NEAREST)
    gt = cv2.resize(gt, IMAGE_SIZE, interpolation=cv2.INTER_NEAREST)

    # Binarisation
    pred = (pred > 0).astype(np.uint8)
    gt = (gt > 0).astype(np.uint8)

    # Tensors
    pred_tensor = torch.tensor(pred, dtype=torch.int32).to(device)
    gt_tensor = torch.tensor(gt, dtype=torch.int32).to(device)

    # M√©triques
    iou_val_lab = iou(pred_tensor, gt_tensor).item()
    dice_val_lab = dice(pred_tensor, gt_tensor).item()
    acc_val = acc(pred_tensor, gt_tensor).item()
    recall_val = recall(pred_tensor, gt_tensor).item()
    cm = conf_matrix(pred_tensor, gt_tensor)
    global_cm += cm

    results.append({
        "image": name,
        "IoU": iou_val_lab,
        "Dice": dice_val_lab,
        "Accuracy": acc_val,
        "Recall": recall_val
    })

# === R√©sum√© des r√©sultats
df = pd.DataFrame(results)
print("\nüìä Moyennes globales :")
print(df.mean(numeric_only=True))

df.to_csv(SAVE_CSV, index=False)
print(f"‚úÖ R√©sultats sauvegard√©s dans : {SAVE_CSV}")

# Matrice de confusion
labels = ["Fond", "Objet"]  # ou ["Fond", "Sph√©ro√Øde"]
cm_np = global_cm.cpu().numpy()

plt.figure(figsize=(5, 4))
sns.heatmap(cm_np, annot=True, fmt="d", cmap="Greens", xticklabels=labels, yticklabels=labels)
plt.xlabel("Pr√©dit")
plt.ylabel("R√©el")
plt.title("Matrice de confusion - Segmentation binaire")
plt.tight_layout()
plt.savefig(SAVE_CM)
plt.show()

print(f"‚úÖ Matrice de confusion sauvegard√©e dans : {SAVE_CM}")

# ==============================================================
# üöÄ 1Ô∏è‚É£  IMPORTS ET CONFIG
# ==============================================================

import os
import sys
from pathlib import Path

import torch
import pytorch_lightning as pl
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from torchmetrics.detection.mean_ap import MeanAveragePrecision

# ‚û§ Tes modules perso (ajuste le chemin)
project_root = Path(os.getcwd()).parent
sys.path.append(str(project_root))


# üìÅ Dossiers de sortie
OUTPUT_DIR = "./output_object_detection_Lab"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ==============================================================
# üöÄ 2Ô∏è‚É£  DEVICE + MODELE
# ==============================================================

# D√©finir le device d'abord !
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚úÖ Device utilis√© : {device}")

# Charger le mod√®le (ajuste le chemin du checkpoint et les param√®tres)
checkpoint_path = "checkpoints/maskrcnn_lab/best_maskrcnn_lab.ckpt"

if not os.path.exists(checkpoint_path):
    raise FileNotFoundError(f"‚ùå Checkpoint introuvable : {checkpoint_path}")

try:
    model_maskrcnn_lab = MaskrcnnModel.load_from_checkpoint(
        checkpoint_path=checkpoint_path,
        model_name="mask_rcnn",
        num_classes=4,
        segmentation_type="mask_rcnn"
    )
    model_maskrcnn_lab.to(device)
    model_maskrcnn_lab.eval()
    print("‚úÖ Mod√®le Mask R-CNN charg√©.")
except Exception as e:
    raise RuntimeError(f"‚ùå Erreur lors du chargement : {e}")

# ==============================================================
# üöÄ 3Ô∏è‚É£  DATA LOADER (exemple ‚Äî adapte √† ton dataset)
# ==============================================================

def collate_fn(batch):
    """Fonction de collation Mask R-CNN LAB"""
    return tuple(zip(*batch))

# D√©commente et adapte :
test_dataset_maskrcnn_lab = CellSegmentationDataset(X_test, M_test, num_classes=4, segmentation_type='mask_rcnn', transform=get_validation_transforms())
test_loader_maskrcnn_lab = torch.utils.data.DataLoader(test_dataset_maskrcnn_lab, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)

print("‚úÖ Dataset et DataLoader configur√©s.")

# ==============================================================
# üöÄ 4Ô∏è‚É£  EVALUATION ET CALCUL mAP
# ==============================================================

metric_map = MeanAveragePrecision(box_format="xyxy", iou_type="segm", class_metrics=True)
metric_map.to(device)

print("‚è≥ √âvaluation en cours...")

with torch.no_grad():
    for images, targets in test_loader_maskrcnn:
        images = [img.to(device) for img in images]
        targets_on_device = [{k: v.to(device) for k, v in t.items()} for t in targets]

        outputs = model_maskrcnn(images)

        preds_formatted = []
        for out in outputs:
            masks = (out["masks"] > 0.5).squeeze(1) if "masks" in out else torch.empty(0, device=device)
            preds_formatted.append({
                "boxes": out["boxes"],
                "scores": out["scores"].float(),
                "labels": out["labels"],
                "masks": masks
            })

        targets_formatted = []
        for tgt in targets_on_device:
            masks = tgt["masks"].squeeze(1) if "masks" in tgt else torch.empty(0, device=device)
            targets_formatted.append({
                "boxes": tgt["boxes"],
                "labels": tgt["labels"],
                "masks": masks
            })

        metric_map.update(preds_formatted, targets_formatted)

final_map_results = metric_map.compute()

print("\n‚úÖ √âvaluation termin√©e.")
print(f"mAP : {final_map_results['map'].item():.4f}")

# ==============================================================
# üöÄ 5Ô∏è‚É£  METRIQUES PAR CLASSE
# ==============================================================

class_labels_map = {1: "monocouche", 2: "aggregation", 3: "spheroide"}

map_per_class_data = {}
if torch.is_tensor(final_map_results['map_per_class']):
    map_per_class_values = final_map_results['map_per_class'].cpu().numpy()
else:
    map_per_class_values = final_map_results['map_per_class']

if isinstance(map_per_class_values, np.ndarray) and map_per_class_values.ndim == 1:
    for i, map_val in enumerate(map_per_class_values):
        class_name = class_labels_map.get(i + 1, f"Classe {i+1}")
        print(f" {class_name}: {map_val:.4f}")
        map_per_class_data[class_name] = float(map_val)

# ==============================================================
# üöÄ 6Ô∏è‚É£  CREATION ET SAUVEGARDE DU TABLEAU ET GRAPHIQUE
# ==============================================================

main_metrics = pd.DataFrame({
    'Metric': [
        'mAP', 'mAP_50', 'mAP_75',
        'mAP_small', 'mAP_medium', 'mAP_large', 'Recall (AR@100)'
    ],
    'Value': [
        final_map_results['map'].item(),
        final_map_results['map_50'].item(),
        final_map_results['map_75'].item(),
        final_map_results['map_small'].item(),
        final_map_results['map_medium'].item(),
        final_map_results['map_large'].item(),
        final_map_results['mar_100'].item()
    ]
})

class_metrics = pd.DataFrame([
    {'Class': cls_name, 'mAP': mAP_val}
    for cls_name, mAP_val in map_per_class_data.items()
])

# ‚û§ Sauvegarde CSV
main_metrics.to_csv(os.path.join(OUTPUT_DIR, "main_metrics_lab.csv"), index=False)
class_metrics.to_csv(os.path.join(OUTPUT_DIR, "class_metrics_lab.csv"), index=False)

# ‚û§ Graphique
plt.figure(figsize=(10, 6))
bars = plt.bar(main_metrics['Metric'], main_metrics['Value'],
               color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',
                      '#9467bd', '#8c564b', '#e377c2'])

for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.3f}', ha='center', va='bottom')

plt.title('Performance du mod√®le Mask R-CNN LAB', fontsize=14)
plt.ylabel('Score', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

plt.savefig(os.path.join(OUTPUT_DIR, 'metrics_plot_lab.png'),
            dpi=300, bbox_inches='tight')
plt.show()

print(f"‚úÖ R√©sultats sauvegard√©s LAB dans : {OUTPUT_DIR}")

visualize_maskrcnn_predictions(
    model=model_maskrcnn_lab,
    test_loader=test_loader_maskrcnn_lab,
    output_dir="./maskrcnn_visualizations_lab",
    num_examples=5,  # Change pour + ou -
    confidence_threshold=0.7
)

"""les valeur de Jaccad et Dice dans les models unet++ et MaskRcnn avec luminanace Lab"""

from torchmetrics import F1Score, JaccardIndex

# ‚û§ Initialiser
dice_metric_maskrcnn_lab = F1Score(task='binary').to(device)  # F1 = Dice pour binaire
jaccard_metric_maskrcnn_lab = JaccardIndex(task='binary').to(device)

with torch.no_grad():
    for i, (images, targets) in enumerate(test_loader_maskrcnn):
        images = [img.to(device) for img in images]
        targets_on_device = [{k: v.to(device) for k, v in t.items()} for t in targets]

        outputs = model_maskrcnn_lab(images)

        preds_formatted = []
        masks_preds = []
        masks_true = []

        for out, tgt in zip(outputs, targets_on_device):
            # Pour metric_map (d√©tection)
            masks = (out["masks"] > 0.5).squeeze(1)
            preds_formatted.append({
                "boxes": out["boxes"],
                "scores": out["scores"].float(),
                "labels": out["labels"],
                "masks": masks
            })

            # Pour Dice/IoU pixel
            if masks.numel() > 0:
                pred_mask = masks.sum(dim=0).clamp(0,1).unsqueeze(0).int()
            else:
                pred_mask = torch.zeros_like(tgt["masks"][0]).unsqueeze(0).int()

            true_mask = tgt["masks"].sum(dim=0).clamp(0,1).unsqueeze(0).int()

            dice_metric_maskrcnn_lab.update(pred_mask, true_mask)
            jaccard_metric_maskrcnn_lab.update(pred_mask, true_mask)

        targets_formatted = [
            {"boxes": tgt["boxes"], "labels": tgt["labels"], "masks": tgt["masks"].squeeze(1)}
            for tgt in targets_on_device
        ]
        metric_map.update(preds_formatted, targets_formatted)

# ‚û§ Apr√®s la boucle :
dice_maskrcnn_lab = dice_metric_maskrcnn_lab.compute().item()
jaccard_maskrcnn_lab = jaccard_metric_maskrcnn_lab.compute().item()

print(f"‚úÖ Mask R-CNN Dice : {dice_maskrcnn_lab:.4f}")
print(f"‚úÖ Mask R-CNN IoU  : {jaccard_maskrcnn_lab:.4f}")

# ‚ö° Cast explicite en float pour √©viter l'erreur de format
dice_unetpp_lab = float(dice_val_lab)
jaccard_unetpp_lab = float(iou_val_lab)
dice_maskrcnn_lab = float(dice_maskrcnn_lab)
jaccard_maskrcnn_lab = float(jaccard_maskrcnn_lab)

# ‚úÖ Utiliser les versions cast√©es
df_comparaison = pd.DataFrame({
    "Model": ["Mask R-CNN", "U-Net++"],
    "Dice (F1 Score)": [dice_maskrcnn_lab, dice_unetpp_lab],
    "Jaccard (IoU)": [jaccard_maskrcnn_lab, jaccard_unetpp_lab
]})

# ‚úÖ Formater seulement les colonnes num√©riques
display(
    df_comparaison.style
    .format({
        "Dice (F1 Score)": "{:.5f}",
        "Jaccard (IoU)": "{:.5f}"
    })
    .set_caption("Comparaison des m√©triques")
)
# ‚úÖ Sauvegarder
SAVE_COMPARISON = os.path.join(SAVE_DIR, "comparison_maskrcnn_unetpp_lab.csv")
df_comparaison.to_csv(SAVE_COMPARISON, index=False)
print(f"\n‚úÖ Tableau de comparaison sauvegard√© : {SAVE_COMPARISON}")

"""la combaraisaon de  unet++ et maskrcnn avec et sans Lab"""

import pandas as pd

results = pd.DataFrame([
    ["Mask R-CNN", "yes", dice_maskrcnn_lab,jaccard_maskrcnn_lab],
    ["Unet++",     "yes", dice_unetpp_lab,  jaccard_unetpp_lab],
    ["Mask R-CNN", "no",  dice_maskrcnn, jaccard_maskrcnn],
    ["Unet++",     "no",  dice_unetpp,    jaccard_unetpp]
], columns=["Model", "Luminance Conversion", "Dice", "Jaccard"])

print(results)

results_path = "/content/drive/MyDrive/Results/comparaison_unet_maskrcnn_rgb_luminance.csv"
results.to_csv(results_path, index=False)
print(f"‚úÖ Tableau sauvegard√© dans : {results_path}")

"""Evaluation dataset validation"""

import pandas as pd
import numpy as np
import torch
from torchmetrics.classification import BinaryJaccardIndex, BinaryF1Score
from torch.utils.data import DataLoader




# üìä Tableau comparatif
results_df = pd.DataFrame([
    ["Mask R-CNN", "yes", dice_maskrcnn_lab, iou_maskrcnn_lab],
    ["Unet++",     "yes", dice_unetpp_lab,     iou_unetpp_lab],
    ["Mask R-CNN", "no",  dice_maskrcnn, iou_maskrcnn],
    ["Unet++",     "no",  dice_unet,     iou_unet]
], columns=["Model", "Luminance Conversion", "Dice", "Jaccard"])

# üßæ Sauvegarde
results_path = "/content/drive/MyDrive/Results/validation_comparaison_luminance.csv"
results_df.to_csv(results_path, index=False)

# üìã Affichage
print("üìä R√©sultats de validation (avec/sans luminance) :")
print(results_df)
print(f"\n‚úÖ Tableau sauvegard√© dans : {results_path}")